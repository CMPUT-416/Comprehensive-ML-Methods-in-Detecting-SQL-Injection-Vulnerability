import xgboost as xgb
import numpy as np
from sklearn.model_selection import KFold

y, ybar = [], []
with open("weights/ybar.txt", 'r') as file:
    for line in file:
        # Use eval to convert string to list
        ybar.append(eval(line.strip()))
with open("weights/y.txt", 'r') as file:
    for line in file:
        # Remove newline character and save label
        y.append(line.strip())

ybar = np.array(ybar).astype(np.int64)
y = np.array(y).astype(np.int64)

data_dmatrix = xgb.DMatrix(data=ybar,label=y)

# List of alpha values to try
alphas = [0, 0.001, 0.01, 0.1, 1, 10, 100]

# Initialize dictionary to store CV results
cv_results = {}

kf = KFold(n_splits=3, shuffle=True, random_state=42)

for alpha in alphas:
    params = {
        'objective': 'reg:squarederror',
        'max_depth': 10,
        'alpha': alpha
    }
    cv_result = xgb.cv(params, data_dmatrix, num_boost_round=50, folds=kf,
                       metrics='rmse', early_stopping_rounds=10, seed=42)
    # Store CV results
    cv_results[alpha] = cv_result

# Print CV results
for alpha, result in cv_results.items():
    print(f"alpha: {alpha}, best RMSE: {result['test-rmse-mean'].min()}")

# Select the best alpha
best_alpha = min(cv_results, key=lambda alpha: cv_results[alpha]['test-rmse-mean'].min())

print(f"Best alpha: {best_alpha}")

params = {
    'objective': 'reg:squarederror',
    'max_depth': 10,
    'alpha': best_alpha
}

model = xgb.train(params, data_dmatrix)

print("MODEL WEIGHTS: ", model.get_score(importance_type="weight"))

weights = model.get_score(importance_type="weight").values()

SUM = sum(weights)
weights = [weight / SUM * len(weights) for weight in weights]
print(weights)