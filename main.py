import os
import numpy as np

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from nltk import word_tokenize
from tqdm import tqdm
from torch import Tensor

from models.knn import KNNModel
from models.nb import NBModel
from models.logReg import LogRegModel
from models.svm import SVMModel
from models.baseline import BaselineModel

class Classifier:
    def __init__(self, cv: int, preprocessor):
        self.preprocessor = preprocessor
        self.cv: int = cv
        self.evals: tuple = ('neg_mean_squared_error',
                             'f1_micro',
                             'balanced_accuracy')
        self.classifiers: list = [
            BaselineModel(),
            LogRegModel(cv=self.cv, cvEval=self.evals),
            KNNModel(cv=self.cv, cvEval=self.evals),
            NBModel(cv=self.cv, cvEval=self.evals),
            SVMModel(cv=self.cv, cvEval=self.evals)]

    def classify(self):
        print("+" * 60)
        print("\t\t\t\t** MODEL LEARNING **")
        print("+" * 60 + '\n')

        print(f">> Training Settings"
              f"\n\t> Method of {self.cv}-fold Cross-validation"
              f"\n\t> Benchmarks of {self.evals}")

        print(f">> Modelling & Predictions")
        for classifier in tqdm(self.classifiers):
            print('\n')
            if classifier == self.classifiers[0]:
                classifier.classify(datasets_test=self.preprocessor.datasets_test)
                continue
            classifier.training(datasets_train=self.preprocessor.datasets_train)
            classifier.test(datasets_test=self.preprocessor.datasets_test)
            classifier.evaluate()

        print('\n' + "+" * 60)
        print("\t\t\t** MODEL PREDICTION ACCOMPLISHED **")
        print("+" * 60 + '\n\n')


class Preprocessor:
    def __init__(self, dirPath="data/", d2vModel=None):
        self.d2vModel = d2vModel
        self.datasets_train = {}
        self.datasets_test = {}
        self.dirPath = dirPath

    def process(self):
        print("+" * 60)
        print("\t\t\t\t** DATA PREPROCESSING **")
        print("+" * 60 + '\n')

        self._dataVectorization()
        data: tuple = self._dataInput()
        self._dataSplitting(data, ratio=0.1)
        self._dataScaling()

        print('\n' + "+" * 60)
        print("\t\t\t** DATA PREPROCESS ACCOMPLISHED **")
        print("+" * 60 + '\n\n')

    def _dataNormalization(self, doc):
        return word_tokenize(doc.strip())

    def _dataVectorization(self, vector_size=100, window=5, min_count=1, workers=4):
        print(f">> Data Vectorization"
              f"\n\t> Method of Word2Vec/Doc2Vec")
        documents = []
        for filename in os.listdir(self.dirPath):
            if filename[-8:] == 'sqli.txt':
                print(f"\t\t* Converting {filename}...")
                filePath = os.path.join(self.dirPath, filename)
                with open(filePath, 'r') as f:
                    for index, dataline in enumerate(f):
                        tokens = self._dataNormalization(dataline)
                        documents.append(TaggedDocument(words=tokens, tags=[index]))
        self.d2vModel = Doc2Vec(documents, vector_size=vector_size, window=window, min_count=min_count, workers=workers)

    def _dataInput(self):
        print(f">> Data Input")
        features_hashtable = []
        labels_hashtable = []

        files = os.listdir(self.dirPath)
        txt_files = [f for f in files if f.endswith('.txt')]
        assert len(txt_files) % 2 == 0
        txt_files.sort()

        for i in range(0, len(txt_files), 2):
            f1 = os.path.join(self.dirPath, txt_files[i])
            f2 = os.path.join(self.dirPath, txt_files[i + 1])
            if f1.split("-")[1] == "sqli.txt":
                feature_file = f1
                label_file = f2
            else:
                feature_file = f2
                label_file = f1

            with open(feature_file, 'r') as fF, open(label_file, 'r') as fL:
                for index, (feature, label) in enumerate(zip(fF, fL)):
                    tokens = self._dataNormalization(feature)
                    vector = self.d2vModel.infer_vector(tokens)
                    features_hashtable.append(vector)
                    labels_hashtable.append(int(label.strip()))
        return np.array(features_hashtable), np.array(labels_hashtable)

    def _dataSplitting(self, datasets: tuple, ratio=0.1):
        print(f">> Data Splitting")
        X_train, X_test, y_train, y_test = train_test_split(datasets[0], datasets[1], test_size=0.1, shuffle=True)
        self.datasets_train = [X_train.astype(float), y_train.astype(int)]
        self.datasets_test = [X_test.astype(float), y_test.astype(int)]
        print(f"\t> Data Loading:"
              f"\n\t      * datasets_train"
              f"\n\t\t        --> X: {self.datasets_train[0].shape}, y: {self.datasets_train[1].shape}"
              f"\n\t      * datasets_test"
              f"\n\t\t        --> X: {self.datasets_test[0].shape}, y: {self.datasets_test[1].shape}")

    def _dataScaling(self):
        print(f">> Data Scaling")
        scaler = StandardScaler().fit(self.datasets_train[0])
        self.datasets_train[0] = scaler.transform(self.datasets_train[0])
        self.datasets_test[0] = scaler.transform(self.datasets_test[0])



if __name__ == '__main__':
    P = Preprocessor(dirPath="data/")
    P.process()
    C = Classifier(cv=3, preprocessor=P)
    C.classify()