
# **CMPUT 416/500 - Machine Learning Methods in Detecting SQL Injection Vulnerability**

| Team Member   | Contact             |
|---------------|---------------------|
| Eden Zhou     | zhou9@ualberta.ca   |
| Jingtong Yang | jyang9@ualberta.ca  |
| Yuzhou Li     | yuzhou5@ualberta.ca |


# Project Overview

> This project focuses on utilizing Machine Learning (ML) models to detect SQL Injection vulnerabilities effectively. We employ five different ML models, each trained on the same dataset, to identify potential SQL Injection risks in SQL semantics. These models include Logistic Regression, Naive Bayes, k-nearest Neighbours and Support Vector Machine. The system is designed to use either individual models or a combination of all five to enhance detection accuracy. With some key features:
- **Multi-Models Detection**: 
  - Utilize a variety of ML models to detect SQL injection vulnerabilities;
  - Combine insights from models for a comprehensive vulnerability assessment;
  - Pretrained models reloaded for stable detection performance;
  
- **Easy-to-use Interface**: 
  - Interactive detector interface, with multiple functions supported;
  - A formal sample provided that presents the practical application of the comprehensive detector;

- **Research Values**:
  - Validate the ability to apply **non-sequential ML models** in detecting SQL injection vulnerabilities;
  - Examine and compare the **performance of various ML models** for the SQL injection detection task;
  - Examine and compare the **effects of different hyperparameters** in learning performance;


# Program Installation
## Basic Settings
- **Environment**: Please build a `Python 3.8+` environment correctly, with `pip` extension prepared;
  
- **Extensions**: Using the following command under the program directory to install dependent libraries to the environment automatically:

      $ pip install -r requirements.txt
  - For data processing: `matplotlib` and `numpy`;
  - For model learning: `nltk`, `scikit-learn`, `gensim`, `joblib` and `torch`;
  - For presentations: `tqdm`, `prettytable` and `flask`;


## How to Use
- **Detector**: 
  - Execute `detector.py` to start, then follow the interactive instructions below to enter a **command option** and continue the session:

        $ python3 Proj/detector_lite.py
  
  - **Interactive Options**:
    - `d` or `detect` - Checks risks in an SQL input using the selected model. 
      - A pop-up instruction will ask you to select a model, ignore it if no needs;
    - `s` or `scan` - Scans a SQL input with all pre-trained models for a comprehensive report. 
      - A weighted comprehensive risk score computed;
    - `f` or `file` - Scans a log file with all pre-trained models for comprehensive reports. 
      - A report table will be print for line by line analysis;
    - `r` or `rebuild` - Rebuilds a model from scratch (Cover the pre-trained stored);
      - Follow the instruction to select the model type you want to rebuild;
      - The process will cover the pre-trained model stored and create new learning diagram;
    - `h` or `help` - Displays the help page;
    - `e` or `exit` - Exits the program;

- **Formal Example**: 
  - Execute `app.py` to start a **web-app demo**:

        $ python3 Proj/app.py
  - **Detector Deployment**:
    1. Enter potential SQL injection semantics to try the deployed detector;
    2. Feedback will be generated regarding the **risk level** of your inputs;
    3. Then a system decision is made to **accept OR deny** your request (for safety considerations);

- **Replication and Reproduction**
  - Execute `classifier.py` to start, classifier is a development version which contains the whole process of **models building** and **hyperparameter find-tuning** from scratch;
  
        $ python3 Proj/classifier.py
  - Some arguments can be added to the command to fit the development demands:

        $ python3 classifier.py [Rebuild/Pretrained:<NEW/PRE>] [Model:<*/0~8>] [CrossValidation:<1~6>]
  - To have replication and reproduction of evaluation diagrams, pre-trained models, and prediction outcomes, you could simply run the following codes

# Data Availability
## Data Input
- Storage: 
  - Refer to the directory of [`Proj/data`](https://github.com/CMPUT-416/Proj/blob/main/data), **three file pairs (three datasets)** can be found;
- Types:
  - Files with filenames end with `-sqli.txt` (e.g., burp-labels.txt) serve data **features**;
  - Files with filenames end with `-labels.txt` (e.g., burp-labels.txt) serve data **labels**;
  - Files for the same dataset should share the **same starts** of filenames. **Self-defined datasets** can be added follow the same style;
- Formats:
  - For **sqli files**, each line in the file should contain one sql injection semantics;
  - For **labels files**, each line in the file should contain one corresponding class label 
    - `1` for **high-risk vulnerable** samples; 
    - `0` for **low-risk non-vulnerable** samples;

## Data Preprocessing
- After reading SQL queries and the corresponding labels from files, tokenization, normalization and vectorization take over the texts, and then organize them into structured numerical vectors.
- Class `Preprocessor` from `preprocessor.py` preprocesses data before modelling, with steps:
  - **1. Tokenization**: 
    - Operation: Convert each SQL query into individual tokens, and remove spaces behind and in front of the expression; 
  - **2. Normalization**: 
    - Operation (Optional): Standardizing the raw text data for faster and more reasonable processing, `StandardScaler` from `sklearn` applied to scale the data;
  - **3. Vectorization**: 
    - Method: `Word2Vec` technique used for values embedding, which converts tokens into numerical vector representations;
    - Operation: `Doc2Vec` model (an improved version of Word2Vec) from `gensim` applied to process the **string-level vectorization**;
    - Dictionary Storage: The **vectorization model** is saved under the directory of [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), named `doc2vec.pkl`. Reuse when predicting;
  - **4. Splitting**:
    - Method: Splitting into training and test sets, having data in sets ready for later model learning;
    - Operation: `train_test_split` method from `sklearn` applied to split the data input into two parts: 
      - Dataset for **Training & Validation (90%)**
      - Dataset for **Test (10%)**


# Model Learning
## Learning Design
  - **Cross-validation**: `3-fold cross-validation` applied, with **one sample model** built in each round;
    - For each round in cross-validation, a formal way of data splitting ratio works as `9:1` OR `6:3:1`, which means:
      - **60%** of all data used for the in-round **training**;
      - **30%** of all data used for the in-round **validation**;
      - **10%** of all data used for the final **test**;
    - The best performance model will be used in **final test**, and stored locally as the **pre-trained model**;  
    

  - **Hyperparameter Fine-tuning**: a systematic hyperparameter fine-tuning strategy is implemented. 
    - The hyperparameter scope is designed by empiricism;
    - The optimal hyperparameter will be reported and applied to test directly;


  - **Model Methods**: each model class provides methods for training, storing, reloading, and predicting;


  - **Evaluation Metrics**: `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy`. 
    - `F1-scores` is a more reasonable standard, that is finally used to determine the best model;
    - Diagrams created by model method `_evaluate()` during the building process are stored in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval) to display the changes in model performance depending on:
      - Hyperparameter Choice vs. **Mean of MSE**;
      - Hyperparameter Choice vs. **Mean of F1-scores**;
      - Hyperparameter Choice vs. **Mean of accuracy**;
    
      *(**Why Mean**: take the average from rounds of cross-validation for each hyperparameter and model.)*


## Model Features
- **[Logistic Regression Model](https://github.com/CMPUT-416/Proj/blob/main/models/logReg.py)**
    - Logistic regression is a statistical model that uses a logistic function to model a binary dependent variable, commonly used for predicting the probability of a certain class or event such as pass/fail;
      - Hyperparameter fine-tuning indicates that the **regularization strength** of `C=500` brings the optimal model performance;
      - Pretrained model named `logistics.pkl` may be found in directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
      - Evaluation Output of diagrams can be found in directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
    - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;


- **[k-nearest Neighbors (k-NN) Model](https://github.com/CMPUT-416/Proj/blob/main/models/knn.py)**
  - k-Nearest Neighbors (k-NN) algorithm is a type of instance-based learning method used for classification and regression that predicts a data point's outcome based on the outcomes of its 'k' closest neighbors in the feature space.
    - Hyperparameter fine-tuning indicates that the **considered neighbor number** of `k=1` brings the optimal model performance;
    - Pretrained model named `knn.pkl` may be found in directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
    - Evaluation Output of diagrams can be found in directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;


- **[Naive Bayes (NBs) Model](https://github.com/CMPUT-416/Proj/blob/main/models/nb.py)**
  - Naive Bayes is a generative model which means it predicts the joint probability, P(x, y), of the features, x, and the target, y, and it assumes that all the features are conditionally independent given the target.
    - Hyperparameter fine-tuning indicates that the **inverse regularization strength** of `α=1000` brings the optimal model performance;
    - Pretrained model named `nb.pkl` may be found in directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
    - Evaluation Output of diagrams can be found in directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;

  
- **[Support Vector Machine (SVM) Model](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py)**
  - Support Vector Machine is a discriminative model which means it predicts the joint probability, P(x, y), of the features, x, and the target, y, and it assumes that all the features are conditionally independent given the target.
    - Hyperparameter fine-tuning indicates that the **intensity controller** of `C=100` brings the optimal model performance;
    - Pretrained model named `svm.pkl` may be found in directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
    - Evaluation Output of diagrams can be found in directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;


- **[Decision Tree Model](https://github.com/CMPUT-416/Proj/blob/main/models/decision_tree.py)**
    - Logistic regression is a statistical model that uses a logistic function to model a binary dependent variable, commonly used for predicting the probability of a certain class or event such as pass/fail;
      - Hyperparameter fine-tuning indicates that the **tree searching depth** of `depth=7` brings the optimal model performance;
      - Pretrained model named `dt.pkl` may be found in directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
      - Evaluation Output of diagrams can be found in directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
    - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;


## Model Performance
|                        | [Logistic Regression]() | [k-Nearest Neighbors]() | [Naive Bayes]() | [Support Vector Machine]() | [Decision Tree]() | []() | 
|------------------------|-------------------------|-------------------------|-----------------|----------------------------|-------------------|------|
| TP                     |                         |                         |                 |                            |                   |      |                      
| FP                     |                         |                         |                 |                            |                   |      |               
| TN                     |                         |                         |                 |                            |                   |      | 
| FN                     |                         |                         |                 |                            |                   |      |                    
| Accuracy               |                         |                         |                 |                            |                   |      |                    
| F1_micro               |                         |                         |                 |                            |                   |      |                       
| F1_macro               |                         |                         |                 |                            |                   |      |                       
| Test Loss              |                         |                         |                 |                            |                   |      |                       
| Optimal Hyperparameter | *C* = 500               | *k* = 1                 | *α* = 1000      | *C* = 100                  | *depth* = 7       |      |                       

# Weighting and Detecting


# Error handling
- Model Selection Validation: The code ensures that the selected machine learning model is within a valid range (0-4). If an invalid model is chosen, the script notifies the user and halts execution, preventing any undefined behavior.
- Command-Line Argument Verification: The script checks for correct command-line arguments for MODE, SELECTION, and CV. If these are missing or incorrect, it defaults to preset values, allowing the program to continue running smoothly without abrupt termination.
- File Existence Check: When loading pre-trained model files, the script checks if these files exist. If a file is not found, it informs the user instead of causing a file-not-found error.

