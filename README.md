
# **CMPUT 416/500 - Machine Learning Methods in Detecting SQL Injection Vulnerability**

| Team Member   | Contact             |
|---------------|---------------------|
| Eden Zhou     | zhou9@ualberta.ca   |
| Jingtong Yang | jyang9@ualberta.ca  |
| Yuzhou Li     | yuzhou5@ualberta.ca |


(TODO: Model Performance test and provide formal data analysis, fill all tables and collect evaluation (hyperparams/cv/weights) for reports)

(TODO: A few diagrams needed - 1. whole process; 2. preprocessing process; 3. multi-model detection process;)

(TODO: More details about the cv indication)

(TODO: Weights and detection)

(TODO: Intro the 2nd level model of xgboost)

(TODO: Error handling)

(TODO: Final check of codes and readme, having coding part done)


# Project Overview

> This project focuses on utilizing Machine Learning (ML) models to detect SQL Injection vulnerabilities effectively. We employ five different ML models, each trained on the same dataset, to identify potential SQL Injection risks in SQL semantics. These models include Logistic Regression, Naive Bayes, k-nearest Neighbours and Support Vector Machine. The system is designed to use either individual models or a combination of all five to enhance detection accuracy. With some key features:
- **Multi-Models Detection**: 
  - Utilize a variety of ML models to detect SQL injection vulnerabilities;
  - Combine insights from models for a comprehensive vulnerability assessment;
  - Pretrained models reloaded for stable detection performance;
  
- **Easy-to-use Interface**: 
  - Interactive detector interface, with multiple functions supported;
  - A formal sample provided that presents the practical application of the comprehensive detector;

- **Research Values**:
  - Validate the ability to apply **non-sequential ML models** in detecting SQL injection vulnerabilities;
  - Examine and compare the **performance of various ML models** for the SQL injection detection task;
  - Examine and compare the **effects of different hyperparameters** in learning performance;
  - Validating the possibility and performance of jointly weighted evaluation using multiple models


![alt text](https://github.com/CMPUT-416/ML-Methods-in-Detecting-SQL-Injection-Vulnerability/blob/main/Architecture.png)


# Program Installation
## Basic Settings
- **Environment**: Please build a `Python 3.8+` environment correctly, with `pip` extension prepared;
  
- **Extensions**: Using the following command under the program directory to install dependent libraries to the environment automatically:

      $ pip install -r requirements.txt
  - For data processing: `matplotlib` and `numpy`;
  - For model learning: `nltk`, `scikit-learn`, `gensim`, `joblib` and `torch`;
  - For presentations: `tqdm`, `prettytable` and `flask`;


## How to Use
- **Detector**: 
  - Execute `detector.py` to start, then follow the interactive instructions below to enter a **command option** and continue the session:

        $ python3 Proj/detector_lite.py
  
  - **Interactive Options**:
    - `d` or `detect` - Checks risks in a **SQL injection input** using a **selected model**. 
      - A pop-up instruction will ask you to select a model, ignore it if no need;
    - `s` or `scan` - Scans a **SQL injection** with **all pre-trained models** for a comprehensive report. 
      - A weighted comprehensive risk score computed;
    - `f` or `file` - Scans a **log file** with **all pre-trained models** for comprehensive reports. 
      - A report table will be printed for line-by-line analysis;
    - `r` or `rebuild` - **Rebuilds a model** from scratch (Covers the pre-trained stored);
      - Follow the instructions to select the model type you want to rebuild;
      - The process will cover the pre-trained model stored and create a new learning diagram;
    -  `a` or `analyze` - developer function for performance analysis;
      - Used to examine and compare the multi-classifier outcomes to the output of every single classifier 
      - And also compare to the ground-truth labels, to research the performance practically and crate a
      - A log file save locally about the analysis outcomes;
    - `h` or `help` - Displays the **help page**;
    - `e` or `exit` - **Exits** the program;

- **Formal Example**: 
  - Execute `app.py` to start a **web-app demo**:

        $ python3 Proj/app.py
  - **Detector Deployment**:
    1. Enter potential SQL injection semantics to try the deployed detector;
    2. Feedback will be generated regarding the **risk level** of your inputs;
    3. Then a system decision is made to **accept OR deny** your request (for safety considerations);

- **Replication and Reproduction**
  - For replication and reproduction purposes, execute `classifier.py` directly. "Classifier" is a development version which contains the whole process of **models building** and **hyperparameter find-tuning** from scratch;
  
        $ python3 Proj/classifier.py
  - This file will **automatically reprocess the whole learning task**, and **regenerate all information needed**: prediction outcomes, performance analysis data/diagrams, tuning/optimization reports, and pre-trained models;
    - ***WARNING***: The process may take hours to run. To speed up the Reproduction process, **avoid retraining the most time-consuming model `GBDT`** by **removing** it from the classifier list at [~Line 65] in `Classifier.py`;
  - Some arguments can be added to the command to fit the development demands:

        $ python3 classifier.py [k:<2~6>]
    - The **third part** of the command may accept a value that determines the **number of folds** in the cross-validation process. Where `k` indicates the **Fold Number** for the cross-validation process;

# Data Availability
## Data Input
- Storage: 
  - Refer to the directory of [`Proj/data`](https://github.com/CMPUT-416/Proj/blob/main/data), **four file pairs (four datasets)** can be found;
- Types:
  - Files with filenames end with `-sqli.txt` (e.g., burp-labels.txt) serve data **features**;
  - Files with filenames end with `-labels.txt` (e.g., burp-labels.txt) serve data **labels**;
  - Files for the same dataset should share the **same starts** of filenames. **Self-defined datasets** can be added following the same style;
- Formats:
  - For **sqli files**, each line in the file should contain one SQL injection semantics;
  - For **labels files**, each line in the file should contain one corresponding class label 
    - `1` for **high-risk vulnerable** samples; 
    - `0` for **low-risk non-vulnerable** samples;

## Data Preprocessing
- After reading SQL queries and the corresponding labels from files, tokenization, normalization and vectorization take over the texts, and then organize them into structured numerical vectors.
- Class `Preprocessor` from `preprocessor.py` preprocesses data before modelling, with steps:
  - **1. Tokenization**: 
    - Operation: Convert each SQL query into individual tokens, and remove spaces behind and in front of the expression; 
  - **2. Normalization**: 
    - Operation (Optional): Standardizing the raw text data for faster and more reasonable processing, `StandardScaler` from `sklearn` applied to scale the data;
  - **3. Vectorization**: 
    - Method: `Word2Vec` technique used for values embedding, which converts tokens into numerical vector representations;
    - Operation: `Doc2Vec` model (an improved version of Word2Vec) from `gensim` applied to process the **string-level vectorization**;
    - Dictionary Storage: The **vectorization model** is saved under the directory of [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), named `doc2vec.pkl`. Reuse when predicting;
  
         > ****Design and Concerns***: Although we have some models (e.g., Naive Bayes) that work well in processing string/token-based data, most of the most famous classification models are designed for numerical (vector-based) inputs. In this way, we selected the vectorization technique of  `Doc2Vec`. Vectorization is a key technique used to convert token expressions into numerical vector representations in Natural/Machine Language Processing. Some other methods like `Bag of Word (BoW)` may bring similar outcomes, but it may **suffer more from `Unknown (UNK)` words**, and it is **more costly** in saving the BoW model. The method of `Word2Vec` may also be applied in this case, but it has some limitations due to the **unbalanced length of input documents/queries** (must apply the second-level filter or take some information retrieval techniques such as `tf-idf` to re-balance the inputs) and maintains the limited relationship between tokens. By contrast, the approach of `Doc2Vec` provides **a more reasonable solution** for semantics with diverse sizes and it can provide a better solution in keeping *between-token dependencies* (which will facilitate the later model learning);



  - **4. Reorganization**:
    - Method: Splitting into training and test sets, having data in sets ready for later model learning;
    - Operation: `train_test_split` method from `sklearn` applied to split the data input into two parts: 
    - Shuffle: Data **random shuffle** applied when various datasets are created;
      - Dataset for **Training & Validation (90%)**
      - Dataset for **Test (10%)**
  
         > ****Design and Concerns***: **Random data shuffle** helps improve model robustness to have more general performance but may also result in **unequal outcomes** for each time of modelling; The process will also **reorganize the data structure** to facilitate the processes of model learning;


# Model Learning
## Learning Design

  - **Model Methods**: 
    - Each model class contains methods for training, storing, reloading, and predicting;
    - A controller approach called `classify(datasets_train, datasets_test, doTraining: bool)` is established in each model to be called from the outers to process the model learning automatically. The model will execute
      - `Training + Prediction` when `doTraining=True`, OR 
      - only `Prediction` when `doTraining=False` with valida model reloaded;

  - **Hyperparameter Fine-tuning**: 
    - A systematic hyperparameter fine-tuning strategy is implemented. The optimal hyperparameter will be reported and applied to test directly;
    - The hyperparameter scope is designed by empiricism, and the selection is processed by loop-examinations;
    - Models with the optimal hyperparameter are trained and saved locally as the pre-trained models;
  
         > ****Design and Concerns***: The Fine-tuning process will examine models based on **different hyperparameters (loop over the empiricism-based hyperparameter selection scope)** under the same environment. The program will follow the same learning design in formal training by using cross-validation and the same pre-processing approaches. The optimal hyperparameter with the corresponding performance (e,g., F-scores) will **be reported**. The model with the best hyperparameter choice is **prepared and saved locally** for later prediction tasks.

  - **Cross-validation**: 
    - `3-fold cross-validation` applied, with **one sample model** built in each round. Data **random shuffle** is applied when folds of cross-validation are created.
    - Based on the outcomes from the previous tests, cross-validation with `3-folds` came with the optimal performance with a reasonable amount of data used for each round of training and validation;
    - For each round in cross-validation, a formal way of data splitting ratio works as `9:1` OR `6:3:1`, which means:
      - **60%** of all data used for the in-round **training**;
      - **30%** of all data used for the in-round **validation**;
      - **10%** of all data used for the final **test**;
    - The model with the best performance will be used in the **final test**, and stored locally as the **pre-trained model**;
  
         > ****Design and Concerns***: The model with the best performance (the best **selected from in-fold models** based on the metric of `F-score`) with the optimal hyperparameter is **saved locally** for later prediction tasks. **Random shuffle** helps improve model robustness to reduce the probability of over-fitting but may result in **unequal outcomes** for each time of processing (users may **not expect to see the same outcomes** at each time of execution).

  - **Evaluation Metrics**:
    - `Mean Squared Error (MSE)`;
    - `F1-scores`: `Macro F1-score` AND `Micro F1-score` (core standard);
    - `balanced accuracy`;
  
         > ****Design and Concerns***: `Mean Squared Error (MSE)` is computed based on the difference between the **model predictions** and the **ground-truth labels**, which is mainly used to indicate model learning; The `F-score` is the **harmonic mean of `precision` and `recall`**, taking both `false positives` and `false negatives` into account. The **higher** the `F1-score` (i.e., `1` means taking **balanced consideration** between `precision` and `recall`), the **more accurate** the model is considered. Compared with other benchmarks, say `accuracy`, `F-scores` is a more reasonable standard (with **much less influence from the unbalanced data** and provides all-around analysis based on the `confusion matrix` (i.e., a table that helps visualize the performance of a classification model by **comparing the actual and predicted labels**)). **Both** benchmarks of `Micro F1-score`, and `Macro F1-score` are reported to have **more general consideration** regarding the performance. In-between, the core standard of `Micro F1-score` is directly used to **determine the optimal model**; `balanced accuracy` is an improved version of `accuracy` which is rebalanced and has less influence from the unbalanced learning data. This standard indicates the **correct prediction percentage** for each model.

  - **Diagram Outputs**: 
    - Diagrams Generation: visualization analysis outcomes are generated by method `_evaluate()` contained in each model during the building process and are stored in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval) to display the changes in model performance depending on:
      - Hyperparameter Choice vs. **Mean of MSE**;
      - Hyperparameter Choice vs. **Mean of F1-scores**;
      - Hyperparameter Choice vs. **Mean of accuracy**;
  
         > ****Design and Concerns***: **Selections of `Hyperparameter Choice`** are displayed on the `x-axis` of diagrams, and the outcomes of **different evaluation standards** corresponding to various hyperparameter choices are shown on the `y-axis`; Taking the `mean` from rounds of cross-validation for each hyperparameter and model to have **more general and precise evaluations**; A special case is that the model `GBDT` has **two hyperparameters for find-tuning**: more diagrams can be found in the directory to indicate the specific selections of the second hyperparameter.

  - **Saving and Reloading**: 
    - **Pre-trained Storages**: All machine learning models and a vectorization model prepared for this task are designed to be saved and reloaded when needed. In [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs) you may find
      - **Pre-trained Vectorization Dictionary Model**: `doc2vec.pkl`;
      - **Pre-trained Machine Learning Models**: `logistics.pkl`, `knn.pkl`, `nb.pkl`, `svm.pkl`, `dt.pkl`, and `gbdt.pkl`;
    - **Update Mechanism**:
      - *(Controlled Automatically)* Each time taking **data pre-processing** will train and save (**replace the exists**) a new `Vectorization Dictionary Model`, named `doc2vec.pkl`;
      - *(General Usage)* Each time calling `build` option in `detector.py` will train and save (**replace the exists**) a new specific `Machine Learning Model` based on the selection made;
      - *(Replication/Reproduction Usage)* Each time running `classifier.py` will redo the whole training and saving process, which means **all pre-trained models will be covered** with the new outcomes;


## Model Features
- **[Logistic Regression Model](https://github.com/CMPUT-416/Proj/blob/main/models/logReg.py)**
  
  > `Logistic Regression` is a statistical model that uses a logistic function to model a binary dependent variable, commonly used for predicting the probability of a certain class or event such as pass/fail;
  
  - Hyperparameter fine-tuning indicates that the **regularization strength** of `C=500` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `logistics.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[k-nearest Neighbors (k-NNs) Model](https://github.com/CMPUT-416/Proj/blob/main/models/knn.py)**
  
  > `k-Nearest Neighbors (k-NNs)` algorithm is a type of instance-based learning method used for classification and regression that predicts a data point's outcome based on the outcomes of its 'k' closest neighbours in the feature space.
  
  - Hyperparameter fine-tuning indicates that the **considered neighbor number** of `k=1` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `knn.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Naive Bayes (NBs) Model](https://github.com/CMPUT-416/Proj/blob/main/models/nb.py)**
  
  > `Naive Bayes (NBs)` is a generative model which means it predicts the joint probability, P(x, y), of the features, x, and the target, y, and it assumes that all the features are conditionally independent given the target.
  
  - Hyperparameter fine-tuning indicates that the **inverse regularization strength** of `α=1000` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `nb.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;

  
- **[Support Vector Machine (SVM) Model](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py)**
  
  > `Support Vector Machine (SVM)` is a discriminative model which means it predicts the joint probability, P(x, y), of the features, x, and the target, y, and it assumes that all the features are conditionally independent given the target.
  
  - Hyperparameter fine-tuning indicates that the **intensity controller** of `C=100` brings the optimal model performance;
  - Pretrained model named `svm.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Decision Tree (DT) Model](https://github.com/CMPUT-416/Proj/blob/main/models/dt.py)**
  
  > `Decision Tree (DT)` is a classification model with a learning process that can be seen as finding an optimal tree structure so that the structure can well describe data characteristics. Each node of the tree corresponds to a feature, and the branching of the node corresponds to a decision made on that feature.
  
  - Hyperparameter fine-tuning indicates that the **tree searching depth** of `depth=7` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `dt.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Gradient Boost Decision Tree (GBDT) Model](https://github.com/CMPUT-416/Proj/blob/main/models/gbdt.py)**
  
  > `Gradient Boosting Decision Tree (GBDT)` is a powerful machine learning algorithm that utilizes an ensemble of decision trees, where each tree is iteratively built to correct the mistakes of the previous trees, effectively improving prediction accuracy by minimizing a loss function via gradient descent.
  
  - Hyperparameter fine-tuning indicates that the **tree searching depth** of `depth=6` and **tree estimator num** of `n=100` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `gbdt.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create (may take minutes to process);
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


## Model Performance
|                        | [Logistic Regression](https://github.com/CMPUT-416/Proj/blob/main/models/logReg.py) | [Naive Bayes](https://github.com/CMPUT-416/Proj/blob/main/models/nb.py) | [k-Nearest Neighbors](https://github.com/CMPUT-416/Proj/blob/main/models/knn.py) | [Support Vector Machine](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py) | [Decision Tree](https://github.com/CMPUT-416/Proj/blob/main/models/dt.py) | [Gradient Boost Decision Tree](https://github.com/CMPUT-416/Proj/blob/main/models/gbdt.py) | 
|------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| TP                     | 340                                                                                 | 241                                                                     | 376                                                                              | 406                                                                                 | 333                                                                       | 386                                                                                        |                      
| FP                     | 47                                                                                  | 122                                                                     | 28                                                                               | 19                                                                                  | 80                                                                        | 30                                                                                         |               
| TN                     | 502                                                                                 | 427                                                                     | 521                                                                              | 530                                                                                 | 469                                                                       | 519                                                                                        | 
| FN                     | 91                                                                                  | 190                                                                     | 55                                                                               | 25                                                                                  | 98                                                                        | 45                                                                                         |                    
| Accuracy               | 0.8592                                                                              | 0.6816                                                                  | 0.9153                                                                           | 0.9551                                                                              | 0.8184                                                                    | 0.9235                                                                                     |                    
| F1_micro               | 0.8592                                                                              | 0.6816                                                                  | 0.9153                                                                           | 0.9551                                                                              | 0.8184                                                                    | 0.9235                                                                                     |                       
| F1_macro               | 0.8552                                                                              | 0.6697                                                                  | 0.9134                                                                           | 0.9544                                                                              | 0.8148                                                                    | 0.9220                                                                                     |                       
| Test Loss              | 0.3923                                                                              | 2.4759                                                                  | 1.0288                                                                           | 0.1474                                                                              | 1.9484                                                                    | 0.2143                                                                                     |                       
| Optimal Hyperparameter | *C* = 100                                                                           | *α* = 50                                                                | *k* = 3                                                                          | *C* = 50                                                                            | *depth* = 10                                                              | *depth* = 5   &   *n* = 30                                                                 |                       



Analysis result from the final test file ``1.txt`` stored in the `Proj/weights` directory, which contains **totally unseen dataset** (p.s., which means we don't even know if they share the same distribution with our learning dataset). Three comprehensive detectors with **different weight determination strategies** are tested and a **single model detector of SVM** is also examined for comparison.




                                                                                      ---------------------------------------draft start here-----------------------------------------------
Notably, the Naive Bayes model has considerably lower accuracy than the others. One plausible reason for this could be the underlying assumption of feature independence that Naive Bayes makes. In real-world datasets, features often exhibit some degree of correlation, which can severely impact the performance of this model.

On the other hand, the Support Vector Machine (SVM) shows the highest accuracy. SVMs are effective in high-dimensional spaces and when there is a clear margin of separation in the data, which could explain its superior performance here. They are also less prone to overfitting, especially with the right choice of the C hyperparameter, as indicated by the optimal C = 50.

The Gradient Boost Decision Tree model also demonstrates high accuracy, slightly below the SVM. This can be attributed to its ensemble approach, where multiple models are combined to improve the prediction. The gradient boosting algorithm focuses on correcting the mistakes of previous models in the sequence, which likely contributes to its high accuracy and relatively lower test loss. The optimal depth and number of estimators (depth = 5 and n = 30) suggest a model complex enough to capture the underlying data patterns but not too complex to overfit.

In contrast, the simple Decision Tree model has a lower accuracy, possibly due to its propensity to overfit, especially when the tree depth is not adequately constrained. The optimal depth of 10 might suggest a relatively complex model, which could be less generalized.

Overall, the distinctive features of these models, such as the assumption of feature independence in Naive Bayes and the powerful, complex ensembling in Gradient Boosted Decision Trees, significantly impact their performance, as reflected in the varied accuracy scores and test losses. Hyperparameter tuning also plays a crucial role in achieving the best model performance, as seen with the SVM and Gradient Boosted Decision Tree models.
                                                                                      ------------------------------------------confusion matrix-------------------------------------------------
For the confusion matrix part, It includes four fundamental elements: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). True Positives and True Negatives represent the correctly identified instances, while False Positives and False Negatives represent the instances where the model erred.

In the model output, the Naive Bayes model has a significantly higher number of False Positives and False Negatives, explaining its lower accuracy and F1 scores. This could be because Naive Bayes may have been too simplistic for the data complexities, leading to misclassification. On the other end, the Support Vector Machine model shows a high number of True Positives and True Negatives with minimal False Positives and False Negatives, indicating a strong ability to distinguish between classes. This robust performance is mirrored in the k-Nearest Neighbors model, which also has high TP and TN counts. The F1 scores, which balance the precision and recall, are particularly telling of the models' performance in relation to the class imbalance. A high F1 score suggests a model is adept at handling both the majority and minority classes effectively, which seems to be the case with the SVM and k-NN models in your data.

The confusion matrix elements directly affect the micro and macro F1 scores. The micro F1 score calculates metrics globally by counting the total true positives, false negatives, and false positives, often giving a higher weight to the majority class. In contrast, the macro F1 score computes the metric independently for each class and then takes the average, thus treating all classes equally. This is why, in a dataset with class imbalances, the macro F1 score might be significantly lower, as observed with the Naive Bayes model, which appears to struggle with class differentiation. The Gradient Boosted Decision Tree, with its balanced approach to error correction in successive iterations, seems to manage these disparities better, as its F1 scores are closer to its accuracy, suggesting a balanced performance across different classes.
                                                                                       ---------------------------------------------draft up--------------------------------------------------------------------------------------------------









|                                   | Ground Truth | [Single Detector of SVM](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py) | [Comprehensive Detector with Empirical Weights Selection](https://github.com/CMPUT-416/Proj/blob/main/detector.py) | [Comprehensive Detector with 2nd-level Model (Linear Regression) Weights Determination](https://github.com/CMPUT-416/Proj/blob/main/reweight1.py) | [Comprehensive Detector with 2nd-level Model (XGBoost) Weights Determination](https://github.com/CMPUT-416/Proj/blob/main/reweight2.py) |
|-----------------------------------|--------------|-------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| VULNERABILITY or HIGH RISK (`~1`) | 121 (56.02)  | 148 (69%)                                                                           | 125 (58%)                                                                                                          | 136 (63%)                                                                                                                                         | 132 (61%)                                                                                                                            |
| MEDIUM RISK (`0.5`)               | -            | 0 (0%)                                                                              | 43 (20%)                                                                                                           | 7 (3%)                                                                                                                                            | 22 (10%)                                                                                                                              |
| LOW RISK (`~0`)                   | 95 (43.98%)  | 68 (32%)                                                                            | 48 (22%)                                                                                                           | 73 (34%)                                                                                                                                          | 62 (29%)                                                                                                                             |
| TOTAL                             | 216          | 216                                                                                 | 216                                                                                                                | 216                                                                                                                                               | 216                                                                                                                                     |
| Weight Selections (for models)    | -            | -                                                                                   | [1, 1, 1, 1, 1, 1]                                                                                                 | [-0.038, -0.124, 2.870, 1.719, 0.252, 1.321]                                                                                                      | [0.500, 0.300, 1.000, 1.900, 1.000, 1.300]                                                                                              |


- ***Reminder***: Due to the data shuffle mechanism during the `data splitting` and `cross-validation`, the training and prediction outcomes may **vary** in different rounds of execution;

Best K found. 
Cross-validation is a statistical method used to estimate the skill of a machine
learning model. Due to the limited data set we have, we are not sure whether a single evaluation of
the model is stable, so we need to divide the sample data into K groups for multiple cross-validations.
The value of K=10 is very common in the field of machine learning. Based on existing experience,
this value has been shown to be neither affected by high bias nor high variance. During our training,
we tried the performance of K from 3 to 10. Our research found that except for a few models
that were less affected by the K value, most models achieved the best performance when K=3
(comprehensive accuracy and time cost).



# Weighting and Detecting



--------------------------------------version 1------------------------------




-Multi-model Comprehensive Detection

In our approach, we process multi-classifier detection by integrating predictions from various models to form a comprehensive vulnerability report. We utilize multiple models because they each focus on different features of the data, leading to a more robust and generalized detection capability. Our process includes two key strategies: empiricism-based weights selection and learning-based weights decision, both relying on second-level Machine Learning Regression Models. Each model starts with an equal base weight, and the total weight is the sum of these individual weights.

-Weights Selection: Empiricism-based Mean Determination
In our empiricism-based weight selection method, we determine the weights for models based on knowledge from prior research. A common technique here is "mean addition," where each model contributes equally to the final detection. This method involves summing the predictions from each model and calculating their mean for a comprehensive prediction. However, this approach can be susceptible to inaccuracies due to outlier predictions from less reliable models.

-Weights Learning: The 2nd Level Machine Learning Model
To enhance our comprehensive detection system, we have implemented advanced learning-based weighting methods. We use two regression models: Linear Regression for its speed and XGBoost for its advanced capabilities. The Linear Regression model is designed to minimize the difference between model predictions and actual labels. For XGBoost, we incorporate multiple "weaker learners" to improve the overall model's performance. Each model is meticulously fine-tuned using cross-validation to ensure its accuracy and effectiveness.

-The Intelligent System
Our final design transforms the classification detector into an intelligent system. This system combines the results from first-level ML detection models with the weights determined by our second-level decision approach. While individual SVM or deep neural network models might perform well on their own, our ensemble approach provides superior robustness and generalization. It focuses on learning features overlooked by single classifiers and offers more comprehensive analysis capabilities for unseen data. Furthermore, our system demonstrates a strong anti-overfitting ability, avoiding excessive reliance on specific features.




------------------------------------End of version 1------------------------------------




------------------------------------version 2------------------------------------

Multi-model Comprehensive Detection

-Integration of multiple models for a robust and generalized detection capability.

-Each model targets different data features, enhancing overall detection accuracy.

-Employs two strategies: empiricism-based weights selection and learning-based weights decision.

-Starts with equal base weights for each model, with total weight being their sum.

Weights Selection: Empiricism-based Mean Determination

-Weights for models determined by prior research and empirical knowledge.

-"Mean addition" technique gives equal weight to each model in final detection.

-Summation of model predictions followed by averaging for comprehensive prediction.

-Approach may be impacted by outlier predictions from less accurate models.

Weights Learning: The 2nd Level Machine Learning Model

-Advanced learning-based weighting methods to enhance detection systems.

-Use of Linear Regression for quick solutions and XGBoost for complex scenarios.

-Linear Regression minimizes gaps between model predictions and actual labels.

-XGBoost improves performance by incorporating multiple "weaker learners."

-Each model is fine-tuned using cross-validation for accuracy and reliability.

The Intelligent System

-Combination of first-level ML detection models with second-level weighting strategies.

-Ensemble approach ensures robustness and generalization beyond single models.

-System learns features missed by single classifiers, enhancing analysis of unseen data.

-Demonstrates strong anti-overfitting ability by not over-relying on specific features.




------------------------------------End of version 2------------------------------------

# Error handling
- Model Selection Validation: The code ensures that the selected machine learning model is within a valid range (0-4). If an invalid model is chosen, the script notifies the user and halts execution, preventing any undefined behavior.
- Command-Line Argument Verification: The script checks for correct command-line arguments for MODE, SELECTION, and CV. If these are missing or incorrect, it defaults to preset values, allowing the program to continue running smoothly without abrupt termination.
- File Existence Check: When loading pre-trained model files, the script checks if these files exist. If a file is not found, it informs the user instead of causing a file-not-found error.


