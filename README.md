
# **CMPUT 416/500 - Machine Learning Methods in Detecting SQL Injection Vulnerability**

| Team Member   | Contact             |
|---------------|---------------------|
| Eden Zhou     | zhou9@ualberta.ca   |
| Jingtong Yang | jyang9@ualberta.ca  |
| Yuzhou Li     | yuzhou5@ualberta.ca |


(TODO: Model Performance test and provide formal data analysis, fill all tables and collect evaluation (hyperparams/cv/weights) for reports)

(TODO: A few diagrams needed - 1. whole process; 2. preprocessing process; 3. multi-model detection process;)

(TODO: More details about the cv indication)

(TODO: Weights and detection)

(TODO: Intro the 2nd level model of xgboost)

(TODO: Error handling)

(TODO: Final check of codes and readme, having coding part done)


# Project Overview

> This project focuses on utilizing Machine Learning (ML) models to detect SQL Injection vulnerabilities effectively. We employ five different ML models, each trained on the same dataset, to identify potential SQL Injection risks in SQL semantics. These models include Logistic Regression, Naive Bayes, k-nearest Neighbours and Support Vector Machine. The system is designed to use either individual models or a combination of all five to enhance detection accuracy. With some key features:
- **Multi-Models Detection**: 
  - Utilize a variety of ML models to detect SQL injection vulnerabilities;
  - Combine insights from models for a comprehensive vulnerability assessment;
  - Pretrained models reloaded for stable detection performance;
  
- **Easy-to-use Interface**: 
  - Interactive detector interface, with multiple functions supported;
  - A formal sample provided that presents the practical application of the comprehensive detector;

- **Research Values**:
  - Validate the ability to apply **non-sequential ML models** in detecting SQL injection vulnerabilities;
  - Examine and compare the **performance of various ML models** for the SQL injection detection task;
  - Examine and compare the **effects of different hyperparameters** in learning performance;
  - Validating the possibility and performance of jointly weighted evaluation using multiple models


![alt text](https://github.com/CMPUT-416/ML-Methods-in-Detecting-SQL-Injection-Vulnerability/blob/main/Architecture.png)


# Program Installation
## Basic Settings
- **Environment**: Please build a `Python 3.8+` environment correctly, with `pip` extension prepared;
  
- **Extensions**: Using the following command under the program directory to install dependent libraries to the environment automatically:

      $ pip install -r requirements.txt
  - For data processing: `matplotlib` and `numpy`;
  - For model learning: `nltk`, `scikit-learn`, `gensim`, `joblib` and `torch`;
  - For presentations: `tqdm`, `prettytable` and `flask`;


## How to Use
- **Detector**: 
  - Execute `detector.py` to start, then follow the interactive instructions below to enter a **command option** and continue the session:

        $ python3 Proj/detector_lite.py
  
  - **Interactive Options**:
    - `d` or `detect` - Checks risks in a **SQL injection input** using a **selected model**. 
      - A pop-up instruction will ask you to select a model, ignore it if no need;
    - `s` or `scan` - Scans a **SQL injection** with **all pre-trained models** for a comprehensive report. 
      - A weighted comprehensive risk score computed;
    - `f` or `file` - Scans a **log file** with **all pre-trained models** for comprehensive reports. 
      - A report table will be printed for line-by-line analysis;
    - `r` or `rebuild` - **Rebuilds a model** from scratch (Covers the pre-trained stored);
      - Follow the instructions to select the model type you want to rebuild;
      - The process will cover the pre-trained model stored and create a new learning diagram;
    -  `a` or `analyze` - developer function for performance analysis;
      - Used to examine and compare the multi-classifier outcomes to the output of every single classifier 
      - And also compare to the ground-truth labels, to research the performance practically and crate a
      - A log file save locally about the analysis outcomes;
    - `h` or `help` - Displays the **help page**;
    - `e` or `exit` - **Exits** the program;

- **Formal Example**: 
  - Execute `app.py` to start a **web-app demo**:

        $ python3 Proj/app.py
  - **Detector Deployment**:
    1. Enter potential SQL injection semantics to try the deployed detector;
    2. Feedback will be generated regarding the **risk level** of your inputs;
    3. Then a system decision is made to **accept OR deny** your request (for safety considerations);

- **Replication and Reproduction**
  - For replication and reproduction purposes, execute `classifier.py` directly. "Classifier" is a development version which contains the whole process of **models building** and **hyperparameter find-tuning** from scratch;
  
        $ python3 Proj/classifier.py
  - This file will **automatically reprocess the whole learning task**, and **regenerate all information needed**: prediction outcomes, performance analysis data/diagrams, tuning/optimization reports, and pre-trained models;
    - ***WARNING***: The process may take hours to run. To speed up the Reproduction process, **avoid retraining the most time-consuming model `GBDT`** by **removing** it from the classifier list at [~Line 65] in `Classifier.py`;
  - Some arguments can be added to the command to fit the development demands:

        $ python3 classifier.py [k:<2~6>]
    - The **third part** of the command may accept a value that determines the **number of folds** in the cross-validation process. Where `k` indicates the **Fold Number** for the cross-validation process;

# Data Availability
## Data Input
- Storage: 
  - Refer to the directory of [`Proj/data`](https://github.com/CMPUT-416/Proj/blob/main/data), **four file pairs (four datasets)** can be found;
- Types:
  - Files with filenames end with `-sqli.txt` (e.g., burp-labels.txt) serve data **features**;
  - Files with filenames end with `-labels.txt` (e.g., burp-labels.txt) serve data **labels**;
  - Files for the same dataset should share the **same starts** of filenames. **Self-defined datasets** can be added following the same style;
- Formats:
  - For **sqli files**, each line in the file should contain one SQL injection semantics;
  - For **labels files**, each line in the file should contain one corresponding class label 
    - `1` for **high-risk vulnerable** samples; 
    - `0` for **low-risk non-vulnerable** samples;

## Data Preprocessing
- After reading SQL queries and the corresponding labels from files, tokenization, normalization and vectorization take over the texts, and then organize them into structured numerical vectors.
- Class `Preprocessor` from `preprocessor.py` preprocesses data before modelling, with steps:
  - **1. Tokenization**: 
    - Operation: Convert each SQL query into individual tokens, and remove spaces behind and in front of the expression; 
  - **2. Normalization**: 
    - Operation (Optional): Standardizing the raw text data for faster and more reasonable processing, `StandardScaler` from `sklearn` applied to scale the data;
  - **3. Vectorization**: 
    - Method: `Word2Vec` technique used for values embedding, which converts tokens into numerical vector representations;
    - Operation: `Doc2Vec` model (an improved version of Word2Vec) from `gensim` applied to process the **string-level vectorization**;
    - Dictionary Storage: The **vectorization model** is saved under the directory of [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), named `doc2vec.pkl`. Reuse when predicting;
  
         > ****Design and Concerns***: Although we have some models (e.g., Naive Bayes) that work well in processing string/token-based data, most of the most famous classification models are designed for numerical (vector-based) inputs. In this way, we selected the vectorization technique of  `Doc2Vec`. Vectorization is a key technique used to convert token expressions into numerical vector representations in Natural/Machine Language Processing. Some other methods like `Bag of Word (BoW)` may bring similar outcomes, but it may **suffer more from `Unknown (UNK)` words**, and it is **more costly** in saving the BoW model. The method of `Word2Vec` may also be applied in this case, but it has some limitations due to the **unbalanced length of input documents/queries** (must apply the second-level filter or take some information retrieval techniques such as `tf-idf` to re-balance the inputs) and maintains the limited relationship between tokens. By contrast, the approach of `Doc2Vec` provides **a more reasonable solution** for semantics with diverse sizes and it can provide a better solution in keeping *between-token dependencies* (which will facilitate the later model learning);



  - **4. Reorganization**:
    - Method: Splitting into training and test sets, having data in sets ready for later model learning;
    - Operation: `train_test_split` method from `sklearn` applied to split the data input into two parts: 
    - Shuffle: Data **random shuffle** applied when various datasets are created;
      - Dataset for **Training & Validation (90%)**
      - Dataset for **Test (10%)**
  
         > ****Design and Concerns***: **Random data shuffle** helps improve model robustness to have more general performance but may also result in **unequal outcomes** for each time of modelling; The process will also **reorganize the data structure** to facilitate the processes of model learning;


# Model Learning
## Learning Design

  - **Model Methods**: 
    - Each model class contains methods for training, storing, reloading, and predicting;
    - A controller approach called `classify(datasets_train, datasets_test, doTraining: bool)` is established in each model to be called from the outers to process the model learning automatically. The model will execute
      - `Training + Prediction` when `doTraining=True`, OR 
      - only `Prediction` when `doTraining=False` with valida model reloaded;

  - **Hyperparameter Fine-tuning**: 
    - A systematic hyperparameter fine-tuning strategy is implemented. The optimal hyperparameter will be reported and applied to test directly;
    - The hyperparameter scope is designed by empiricism, and the selection is processed by loop-examinations;
    - Models with the optimal hyperparameter are trained and saved locally as the pre-trained models;
  
         > ****Design and Concerns***: The Fine-tuning process will examine models based on **different hyperparameters (loop over the empiricism-based hyperparameter selection scope)** under the same environment. The program will follow the same learning design in formal training by using cross-validation and the same pre-processing approaches. The optimal hyperparameter with the corresponding performance (e,g., F-scores) will **be reported**. The model with the best hyperparameter choice is **prepared and saved locally** for later prediction tasks.

  - **Cross-validation**: 
    - `3-fold cross-validation` applied, with **one sample model** built in each round. Data **random shuffle** is applied when folds of cross-validation are created.
    - Based on the outcomes from the previous tests, cross-validation with `3-folds` came with the optimal performance with a reasonable amount of data used for each round of training and validation;
    - For each round in cross-validation, a formal way of data splitting ratio works as `9:1` OR `6:3:1`, which means:
      - **60%** of all data used for the in-round **training**;
      - **30%** of all data used for the in-round **validation**;
      - **10%** of all data used for the final **test**;
    - The model with the best performance will be used in the **final test**, and stored locally as the **pre-trained model**;
  
         > ****Design and Concerns***: The model with the best performance (the best **selected from in-fold models** based on the metric of `F-score`) with the optimal hyperparameter is **saved locally** for later prediction tasks. **Random shuffle** helps improve model robustness to reduce the probability of over-fitting but may result in **unequal outcomes** for each time of processing (users may **not expect to see the same outcomes** at each time of execution).

  - **Evaluation Metrics**:
    - `Mean Squared Error (MSE)`;
    - `F1-scores`: `Macro F1-score` AND `Micro F1-score` (core standard);
    - `balanced accuracy`;
  
         > ****Design and Concerns***: `Mean Squared Error (MSE)` is computed based on the difference between the **model predictions** and the **ground-truth labels**, which is mainly used to indicate model learning; The `F-score` is the **harmonic mean of `precision` and `recall`**, taking both `false positives` and `false negatives` into account. The **higher** the `F1-score` (i.e., `1` means taking **balanced consideration** between `precision` and `recall`), the **more accurate** the model is considered. Compared with other benchmarks, say `accuracy`, `F-scores` is a more reasonable standard (with **much less influence from the unbalanced data** and provides all-around analysis based on the `confusion matrix` (i.e., a table that helps visualize the performance of a classification model by **comparing the actual and predicted labels**)). **Both** benchmarks of `Micro F1-score`, and `Macro F1-score` are reported to have **more general consideration** regarding the performance. In-between, the core standard of `Micro F1-score` is directly used to **determine the optimal model**; `balanced accuracy` is an improved version of `accuracy` which is rebalanced and has less influence from the unbalanced learning data. This standard indicates the **correct prediction percentage** for each model.

  - **Diagram Outputs**: 
    - Diagrams Generation: visualization analysis outcomes are generated by method `_evaluate()` contained in each model during the building process and are stored in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval) to display the changes in model performance depending on:
      - Hyperparameter Choice vs. **Mean of MSE**;
      - Hyperparameter Choice vs. **Mean of F1-scores**;
      - Hyperparameter Choice vs. **Mean of accuracy**;
  
         > ****Design and Concerns***: **Selections of `Hyperparameter Choice`** are displayed on the `x-axis` of diagrams, and the outcomes of **different evaluation standards** corresponding to various hyperparameter choices are shown on the `y-axis`; Taking the `mean` from rounds of cross-validation for each hyperparameter and model to have **more general and precise evaluations**; A special case is that the model `GBDT` has **two hyperparameters for find-tuning**: more diagrams can be found in the directory to indicate the specific selections of the second hyperparameter.

  - **Saving and Reloading**: 
    - **Pre-trained Storages**: All machine learning models and a vectorization model prepared for this task are designed to be saved and reloaded when needed. In [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs) you may find
      - **Pre-trained Vectorization Dictionary Model**: `doc2vec.pkl`;
      - **Pre-trained Machine Learning Models**: `logistics.pkl`, `knn.pkl`, `nb.pkl`, `svm.pkl`, `dt.pkl`, and `gbdt.pkl`;
    - **Update Mechanism**:
      - *(Controlled Automatically)* Each time taking **data pre-processing** will train and save (**replace the exists**) a new `Vectorization Dictionary Model`, named `doc2vec.pkl`;
      - *(General Usage)* Each time calling `build` option in `detector.py` will train and save (**replace the exists**) a new specific `Machine Learning Model` based on the selection made;
      - *(Replication/Reproduction Usage)* Each time running `classifier.py` will redo the whole training and saving process, which means **all pre-trained models will be covered** with the new outcomes;


## Model Features
- **[Logistic Regression Model](https://github.com/CMPUT-416/Proj/blob/main/models/logReg.py)**
  
  > `Logistic Regression` is a statistical model that uses a logistic function to model a binary dependent variable, commonly used for predicting the probability of a certain class or event such as pass/fail;
  
  - Hyperparameter fine-tuning indicates that the **regularization strength** of `C=500` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `logistics.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[k-nearest Neighbors (k-NNs) Model](https://github.com/CMPUT-416/Proj/blob/main/models/knn.py)**
  
  > `k-Nearest Neighbors (k-NNs)` algorithm is a type of instance-based learning method used for classification and regression that predicts a data point's outcome based on the outcomes of its 'k' closest neighbours in the feature space.
  
  - Hyperparameter fine-tuning indicates that the **considered neighbor number** of `k=1` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `knn.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Naive Bayes (NBs) Model](https://github.com/CMPUT-416/Proj/blob/main/models/nb.py)**
  
  > `Naive Bayes (NBs)` is a generative model which means it predicts the joint probability, P(x, y), of the features, x, and the target, y, and it assumes that all the features are conditionally independent given the target.
  
  - Hyperparameter fine-tuning indicates that the **inverse regularization strength** of `α=1000` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `nb.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;

  
- **[Support Vector Machine (SVM) Model](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py)**
  
  > `Support Vector Machine (SVM)` is a discriminative model which means it predicts the joint probability, P(x, y), of the features, x, and the target, y, and it assumes that all the features are conditionally independent given the target.
  
  - Hyperparameter fine-tuning indicates that the **intensity controller** of `C=100` brings the optimal model performance;
  - Pretrained model named `svm.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Decision Tree (DT) Model](https://github.com/CMPUT-416/Proj/blob/main/models/dt.py)**
  
  > `Decision Tree (DT)` is a classification model with a learning process that can be seen as finding an optimal tree structure so that the structure can well describe data characteristics. Each node of the tree corresponds to a feature, and the branching of the node corresponds to a decision made on that feature.
  
  - Hyperparameter fine-tuning indicates that the **tree searching depth** of `depth=7` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `dt.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Gradient Boost Decision Tree (GBDT) Model](https://github.com/CMPUT-416/Proj/blob/main/models/gbdt.py)**
  
  > `Gradient Boosting Decision Tree (GBDT)` is a powerful machine learning algorithm that utilizes an ensemble of decision trees, where each tree is iteratively built to correct the mistakes of the previous trees, effectively improving prediction accuracy by minimizing a loss function via gradient descent.
  
  - Hyperparameter fine-tuning indicates that the **tree searching depth** of `depth=6` and **tree estimator num** of `n=100` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `gbdt.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create (may take minutes to process);
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


## Model Performance
|                        | [Logistic Regression](https://github.com/CMPUT-416/Proj/blob/main/models/logReg.py) | [Naive Bayes](https://github.com/CMPUT-416/Proj/blob/main/models/nb.py) | [k-Nearest Neighbors](https://github.com/CMPUT-416/Proj/blob/main/models/knn.py) | [Support Vector Machine](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py) | [Decision Tree](https://github.com/CMPUT-416/Proj/blob/main/models/dt.py) | [Gradient Boost Decision Tree](https://github.com/CMPUT-416/Proj/blob/main/models/gbdt.py) | 
|------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| TP                     | 380                                                                                 | 302                                                                     | 411                                                                              | 440                                                                                 | 361                                                                       | 409                                                                                        |                      
| FP                     | 51                                                                                  | 78                                                                      | 14                                                                               | 6                                                                                   | 70                                                                        | 21                                                                                         |               
| TN                     | 373                                                                                 | 346                                                                     | 410                                                                              | 418                                                                                 | 354                                                                       | 403                                                                                        | 
| FN                     | 76                                                                                  | 154                                                                     | 45                                                                               | 16                                                                                  | 95                                                                        | 47                                                                                         |                    
| Accuracy               | 0.8556818181818182                                                                  | 0.7363636363636363                                                      | 0.9329545454545455                                                               | 0.975                                                                               | 0.8125                                                                    | 0.9227272727272727                                                                         |                    
| F1_micro               | 0.8556818181818182                                                                  | 0.7363636363636363                                                      | 0.9329545454545455                                                               | 0.975                                                                               | 0.8125                                                                    | 0.9227272727272727                                                                         |                       
| F1_macro               | 0.8556726859008382                                                                  | 0.7357028935976304                                                      | 0.9329544588771421                                                               | 0.9749843652282677                                                                  | 0.8124881352254985                                                        | 0.9227236803363792                                                                         |                       
| Test Loss              | 0.3205                                                                              | 0.6677                                                                  | 0.7897                                                                           | 0.0778                                                                              | 1.6694                                                                    | 0.1900                                                                                     |                       
| Optimal Hyperparameter | *C* = 500                                                                           | *α* = 100                                                               | *k* = 3                                                                          | *C* = 500                                                                           | *depth* = 9                                                               | *depth* = 5   &   *n* = 30                                                                 |                       



|                                   | Ground Truth | [Comprehensive Detector with Empirica Weights Selection]() | [Comprehensive Detector with 2nd-level Model (Linear Regression) Weights Determination](https://github.com/CMPUT-416/Proj/blob/main/reweight1.py) | [Comprehensive Detector with 2nd-level Model (XGBoost) Weights Determination](https://github.com/CMPUT-416/Proj/blob/main/reweight2.py) |
|-----------------------------------|--------------|------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| VULNERABILITY or HIGH RISK (`~1`) | 121 (56.02)  | 136 (62.96%)                                               | 146 (67.59%)                                                                                                                                      | 145 (67.13%)                                                                                                                            |
| MEDIUM RISK (`0.5`)               | -            | 45 (20.83%)                                                | 2 (0.93%)                                                                                                                                         | 21 (9.72%)                                                                                                                              |
| LOW RISK (`~0`)                   | 95 (43.98%)  | 35 (16.20%)                                                | 68 (31.48%)                                                                                                                                       | 50 (23.15%)                                                                                                                             |
| TOTAL                             | 216          | 216                                                        | 216                                                                                                                                               | 216                                                                                                                                     |
| Weight Selections (for models)    | -            | [1, 1, 1, 1, 1, 1]                                         | [-0.292, 0.062, 3.365, 1.206, 0.159, 1.501]                                                                                                       | [1.222, 1.114, 0.359, 0.862, 1.293, 1.150]                                                                                              |


- ***Reminder***: Due to the data shuffle mechanism during the `data splitting` and `cross-validation`, the training and prediction outcomes may **vary** in different rounds of execution;

Overrall result:
Dt- best performanceat cv= 3, acc=0.87
knn-best performanceat cv = 4, acc=0.92
logReg-best performance at cv =3, acc=0.88
Nb-best performance at cv =5, acc=0.74
SVm-best performance at cv =[1,2,3,4,6,7], acc around 0.97 (cv not influence much)
GBDT-best performance at cv = [3,6,7],acc around 0.94

based on the examination, use of CV = 3 could give the most accurate result.




# Weighting and Detecting

# Error handling
- Model Selection Validation: The code ensures that the selected machine learning model is within a valid range (0-4). If an invalid model is chosen, the script notifies the user and halts execution, preventing any undefined behavior.
- Command-Line Argument Verification: The script checks for correct command-line arguments for MODE, SELECTION, and CV. If these are missing or incorrect, it defaults to preset values, allowing the program to continue running smoothly without abrupt termination.
- File Existence Check: When loading pre-trained model files, the script checks if these files exist. If a file is not found, it informs the user instead of causing a file-not-found error.


