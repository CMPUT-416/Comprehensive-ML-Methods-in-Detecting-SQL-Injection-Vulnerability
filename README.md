
# **CMPUT 416/500 - Machine Learning Methods in Detecting SQL Injection Vulnerability**

| Team Member   | Contact             |
|---------------|---------------------|
| Eden Zhou     | zhou9@ualberta.ca   |
| Jingtong Yang | jyang9@ualberta.ca  |
| Yuzhou Li     | yuzhou5@ualberta.ca |


# Project Overview

> This project focuses on utilizing Machine Learning (ML) models to detect SQL Injection vulnerabilities effectively. We employ five different ML models, each trained on the same dataset, to identify potential SQL Injection risks in SQL semantics. These models include Logistic Regression, Naive Bayes, k-nearest Neighbours and Support Vector Machine. The system is designed to use either individual models or a combination of all five to enhance detection accuracy. With some key features:
- **Multi-Models Detection**: 
  - Utilize a variety of ML models to detect SQL injection vulnerabilities;
  - Combine insights from models for a comprehensive vulnerability assessment;
  - Pretrained models reloaded for stable detection performance;
  
- **Easy-to-use Interface**: 
  - Interactive detector interface, with multiple functions supported;
  - A formal sample provided that presents the practical application of the comprehensive detector;

- **Research Values**:
  - Validate the ability to apply **non-sequential ML models** in detecting SQL injection vulnerabilities;
  - Examine and compare the **performance of various ML models** for the SQL injection detection task;
  - Examine and compare the **effects of different hyperparameters** in learning performance;


# Program Installation
## Basic Settings
- **Environment**: Please build a `Python 3.8+` environment correctly, with `pip` extension prepared;
  
- **Extensions**: Using the following command under the program directory to install dependent libraries to the environment automatically:

      $ pip install -r requirements.txt
  - For data processing: `matplotlib` and `numpy`;
  - For model learning: `nltk`, `scikit-learn`, `gensim`, `joblib` and `torch`;
  - For presentations: `tqdm`, `prettytable` and `flask`;


## How to Use
- **Detector**: 
  - Execute `detector_lite.py` to start, then follow the interactive instructions below to enter a **command option** and continue the session:

        $ python3 Proj/detector_lite.py
  
  - **Interactive Options**:
    - `d` or `detect` - Checks risks in an SQL input using the selected model. A pop-up instruction will ask you to select a model, ignore it if no needs;
    - `s` or `scan` - Scans a SQL input with all pre-trained models for a comprehensive report. A weighted comprehensive risk score computed;
    - `f` or `file` - Scans a log file with all pre-trained models for comprehensive reports. A report table will be print for line by line analysis;
    - `r` or `rebuild` - Rebuilds a model from scratch (Cover the pre-trained stored);
    - `h` or `help` - Displays the help page;
    - `e` or `exit` - Exits the program;

- **Formal Example**: 
  - Execute `app.py` to start a **web-app demo**:

        $ python3 Proj/app.py
  - **Detector Deployment**:
    1. Enter potential SQL injection semantics to try the deployed detector;
    2. Feedback will be generated regarding the **risk level** of your inputs;
    3. Then a system decision is made to *accept OR deny* your request (for safety considerations);


# Data Availability
## Data Input

- Storage: 
  - Refer to the directory of `Proj/data`, **three pairs of files (three datasets)** can be found;
- Types:
  - Files with filenames end with `-sqli.txt` (e.g., burp-labels.txt) serve data **features**;
  - Files with filenames end with `-labels.txt` (e.g., burp-labels.txt) serve data **labels**;
  - Files for the same dataset should share the **same starts** of filenames. You could add **self-defined datasets** with the same style;
- Formats:
  - For **sqli files**, each line in the file should contain one sql injection semantics;
  - For **labels files**, each line in the file should contain one corresponding class label (1=vulnerable, 0=non-vulnerable);

## Data Preprocessing
- Data Input and Organization: reads SQL queries and their corresponding labels from files, normalizes the texts, and organizes them into structured vectors.
- Class `Preprocessor` from `preprocessor.py` preprocesses data before modelling with steps of:
  - **1. Tokenization**: 
    - Operation: Convert each SQL query into individual tokens, and remove spaces behind and in front of the expression; 
  - **2. Normalization**: 
    - Operation (Optional): Standardizing the raw text data for faster and more reasonable processing, `StandardScaler` from `sklearn` applied to scale the data;
  - **3. Vectorization**: 
    - Method: `Word2Vec` technique used for values embedding, which converts tokens into numerical vector representations;
    - Operation: `Doc2Vec` model (an improved version of Word2Vec) from `gensim` applied to process the **string-level vectorization**;
    - Storage: The dictionary of vectorization model is saved under the directory of `Proj/models`, named `doc2vec.pkl`;
  - **4. Splitting**:
    - Method: Splitting into training and test sets, having data in sets ready for later model learning;
    - Operation: `train_test_split` method from `sklearn` applied to split the data input into two parts: 
      - Dataset for **Training & Validation (90%)**
      - Dataset for **Test (10%)**


# Model Learning
## Learning Design
  - Cross-validation: `3-fold cross-validation` applied, with **one sample model** built in each round;
    - For each round in cross-validation, a formal way of data splitting ratio works as `9:1` OR `6:3:1`, which means:
      - **60%** of all data used for the in-round **training**;
      - **30%** of all data used for the in-round **validation**;
      - **10%** of all data used for the final **test**;
    - The best performance model will be used in **final test**, and stored locally as the **pre-trained model**;  
  - Hyper-parameter Fine-tuning: 
  - Evaluation Metrics: `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy`'. 
    - `F1-scores` is a more reasonable standard, that is finally used to determine the best model;
  - **Performance Diagrams** created by the model method of `_evaluate()` during the building process are stored in the directory `Proj/eval` to display the changes in model performance depending on:
    - Hyperparameter Choice vs. Mean of MSE;
    - Hyperparameter Choice vs. Mean of F1-scores;
    - Hyperparameter Choice vs. Mean of accuracy;

## Model Features
- **[Logistic Regression Model](https://github.com/CMPUT-416/Proj/blob/main/models/logReg.py)**
  -  
  - Features:
    - Utilizes cross-validation for optimizing model performance.
    - Hyperparameter tuning to find the optimal 'C' value (regularization strength).
    - Evaluates performance using metrics like 
  - Usage: The LogRegModel class provides methods for training, storing, reloading, and predicting, tailored for SQL injection vulnerability detection using the Logistic Regression algorithm.


- **[Naive Bayes (NBs) Model](https://github.com/CMPUT-416/Proj/blob/main/models/nb.py)**
  - Features:
    - Hyperparameter tuning to find the optimal alpha (\(\alpha\)) value for smoothing.
    - Evaluate performance using metrics like Mean Squared Error (MSE), accuracy, and balanced accuracy.
  - Usage: The NBModel class is designed for training, storing, reloading, and making predictions, tailored to SQL injection vulnerability detection using the Naive Bayes algorithm.


- **[k-nearest Neighbors (k-NN) Model](https://github.com/CMPUT-416/Proj/blob/main/models/knn.py)**
  - Purpose: Implements a k-nearest Neighbor model for detecting SQL injection risks.
  - Features:
    - Cross-validation for model tuning.
    - Hyperparameter optimization to find the best k value.
    - Performance evaluation using F1-score and balanced accuracy.
  -  Usage: The KNNModel class provides methods for training, storing, reloading, and predicting using the k-nearest Neighbor algorithm.


- **[Support Vector Machine (SVM) Model](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py)**
  - Purpose: Implements a Support Vector Machine (SVM) model for detecting SQL injection risks.
  - Features:
    - Employs cross-validation to optimize model performance.
    - Hyperparameter tuning to identify the best 'C' value (regularization parameter).
    - Performance evaluation using metrics like Mean Squared Error (MSE), F1-score, and balanced accuracy.
  - Usage: The SVMModel class is designed for training, storing, reloading, and predicting, specifically for SQL injection vulnerability detection using the SVM algorithm.

## Model Performance
|                        | [Logistic Regression]() | [k-Nearest Neighbors]() | [Naive Bayes]() | [Support Vector Machine]() | [Decision Tree]() | []() | 
|------------------------|-------------------------|-------------------------|-----------------|----------------------------|-------------------|------|
| TP                     |                         |                         |                 |                            |                   |      |                      
| FP                     |                         |                         |                 |                            |                   |      |               
| TN                     |                         |                         |                 |                            |                   |      | 
| FN                     |                         |                         |                 |                            |                   |      |                    
| Accuracy               |                         |                         |                 |                            |                   |      |                    
| F1_micro               |                         |                         |                 |                            |                   |      |                       
| F1_macro               |                         |                         |                 |                            |                   |      |                       
| Test Loss              |                         |                         |                 |                            |                   |      |                       
| Optimal Hyperparameter |                         |                         |                 |                            |                   |      |                       

## Model Learning
- The Classifier class (detector.py) trains various machine learning models on the preprocessed data. It supports both new training and using pre-trained models.
  - Initialization: It's set up with cross-validation settings, a preprocessor for data, a list of machine learning models, and evaluation metrics.
  - Classification Process: The class manages the training of models in 'NEW' mode or uses pre-trained models in 'PRE' mode. It iterates through each model, applying them to the dataset for training or prediction.
  - Integration with Preprocessor: Relies on preprocessed data (vectorized and split into training and testing sets) from the Preprocessor class for model training and testing.
  - Flexibility with Multiple Models: Capable of handling various machine learning models, allowing for flexibility and comparison of different algorithms.

# Error handling
- Model Selection Validation: The code ensures that the selected machine learning model is within a valid range (0-4). If an invalid model is chosen, the script notifies the user and halts execution, preventing any undefined behavior.
- Command-Line Argument Verification: The script checks for correct command-line arguments for MODE, SELECTION, and CV. If these are missing or incorrect, it defaults to preset values, allowing the program to continue running smoothly without abrupt termination.
- File Existence Check: When loading pre-trained model files, the script checks if these files exist. If a file is not found, it informs the user instead of causing a file-not-found error.

# Replication and Reproduction