

1 mark: properly formatted README file

2 marks: instructions to reproduce/replicate results

2 marks: data availability

5 marks: result replication/reproduction


# **CMPUT 416/500 - Machine Learning Methods in Detecting SQL Injection Vulnerability**

| Team Member | Contact           |
|-------------|-------------------|
| Eden Zhou   | zhou9@ualberta.ca |
|             |                   |
|             |                   |

- [Part 1 - Project Overview](#part-1---project-overview)
- [Part 2 - Data Availability](#part-2---data-availability)
  * [2.1 Data Input](#21-data-input)
    + [2.1.1 Data Preprocessing](#211-data-preprocessing)
    + [2.1.2 Data Embedding](#212-data-embedding)
    + [2.1.3 Data Splitting](#213-data-splitting)
  * [2.2 Data Output](#22-data-output)
    + [2.2.1 Prediction Outcomes](#221-prediction-outcomes)
    + [2.2.2 Diagram Saving](#222-diagram-saving)
  * [2.3 Data Structures](#23-data-structures)
  * [2.4 Data Analysis](#24-data-analysis)
- [Part 3 - Program Execution](#part-3---program-execution)
  * [3.1 Environment Setup](#31-environment-setup)
    + [3.1.1 Compiling Environment](#311-compiling-environment)
    + [3.1.2 Extension Dependency](#312-extension-dependency)
  * [3.2 Replication Instruction](#32-replication-instruction)
    + [3.2.1 Command-based Execution](#321-command-based-execution)
    + [3.2.2 Script-based Execution](#322-script-based-execution)
- [Part 4 - Learning Designs](#part-4---learning-designs)
  * [4.1 Model Construction](#41-model-construction)
    + [4.1.1 Machine Learning Models](#411-machine-learning-models)
    + [4.1.2 Deep Learning Models](#412-deep-learning-models)
  * [4.2 Learning Patterns](#42-learning-patterns)
  * [4.3 Storing and Reloading](#43-storing-and-reloading)
    + [4.3.1 Model Storing](#431-model-storing)
    + [4.3.2 Model Reloading](#432-model-reloading)
    + [4.3.3 Model Re-training](#433-model-re-training)
  * [4.4 Benchmarks and Analysis](#44-benchmarks-and-analysis)
- [Part 5 - Performance Comparisons](#part-5---performance-comparisons)
  * [5.1 Word2Vec-based Model Outcomes](#51-word2vec-based-model-outcomes)
  * [5.2 BoW-based Model Outcomes](#52-bow-based-model-outcomes)
- [Part 6 - Error Handling](#part-6---error-handling)
- [Part 7 - References](#part-7---references)
  * [7.1 Reusing Claim](#71-reusing-claim)
  * [7.2 External Reference](#72-external-reference)


# Part 1 - Project Overview
> 
> ...


# Part 2 - Data Availability
## 2.1 Data Input
- By default, the program assumes the input training data files presented in the `/data` directory of the program package;
- And the original data should be stored in a file in the format of `.json`;
- Users may define their own input path (for training) by changing **the first bracket part** of the training command;
- Users may define their own input path (for test) by changing **the second bracket part** of the prediction command;

### 2.1.1 Data Preprocessing
### 2.1.2 Data Embedding
### 2.1.3 Data Splitting

## 2.2 Data Output
- Commonly, all the model data learned from model training will be stored in `.tsv` files in the program package;
  - Each TSV file contains data from an original JSON file, and it has **three** columns created: 
    - `type`, the type of the dataline, as "centroid" or "idf";
    - `category/term`, the category type if type="centroid" / the term in string if type="idf";
    - `categoryCentroid/termIDF`, category centroid represent in string (for each token and value) if type="centroid" / the token idf value if type="idf";
- Users may define their own output path by changing **the second bracket part** of the training command;
- Commonly, the file created in training will be used for model test in prediction, defined by **the first bracket part** of the prediction command;
### 2.2.1 Prediction Outcomes
### 2.2.2 Diagram Saving

## 2.3 Data Structures
- Raw data structure after JSON loading: a list contains multiple dictionaries, and each dictionary holds keys of doc, and text, each doc is considered as a data sample:
  - **Raw Input** >> List of dicts, \[{category_1:"XXX", text_1:"XX XX XXX""}, {category_2:"XXX", text_2:"XX XX XXX""}, ...]


- Rearranged training data structure: we assign each doc sample with and docID, this can be used as the key to take out the relevant data:
  - **Training/Test Data** >> Dict of nested-lists, {docID_1: \[docCategory_1, \[token_1_1, token_1_2, ...], docID_2:\[docCategory_2, \[token_2_1, token_2_2, ...]], ...}


- The model probability data structures for dara learned in training:
  - **Category Centroid** >> Nested-dicts, {category_1: {token_1_1:tokenWeight_1_1, token_1_2:tokenWeight_1_2, ...}, category_2: {token_2_1:tokenWeight_2_1, ...}, ...}
  - **Token IDF** >> Hashtable, {token_1:tokenIDF_1, token_2:0.tokenIDF_2, ...}


- The model predictions for each sample doc will be recorded as:
  - **Doc Prediction** >> Dict, {testDocId_1:"category", testDocId_2:"category", ...}


- Distance recording and ranking data structures:
  - **Distance Recorder** >> Dict, {category_1:distToTestDocInInt_1, category_2:distToTestDocInInt_2, ...}
  - **Distance Ranker** >> MinHeap, \[(distToTestDocInInt_1, categoryCentroid_1), (distToTestDocInInt_2, categoryCentroid_2), ...]


## 2.4 Data Analysis
- The model prediction contains a method called self.modelAnalyze which will be call automatically to evaluate the model predictions;
- Analysis results needed will be printed to `STDOUT` channel:
  - Class-level: `true positive`, `false positive`, `false negative`, `true negative`;
  - Class-level: `precision`, `recall` and `F1 scores`;
  - Overall: `micro-averaged F1` & `macro-averaged F1` for the entire test set;


# Part 3 - Program Execution
## 3.1 Environment Setup
### 3.1.1 Compiling Environment
- A properly built **Python environment** is mandatory for executing both programs
  
   - **Original Development Environment**: `Python 3.10`;

<<<<<<< Updated upstream
### 3.1.2 Extension Dependency
- with the following **libraries** required under the environment:
=======
- **Research Values**:
  - Validate the ability to apply **non-sequential ML models** in detecting SQL injection vulnerabilities;
  - Examine and compare the **performance of various ML models** for the SQL injection detection task;
  - Examine and compare the **effects of different hyperparameters** in learning performance;
  - Validating the possibility and performance of jointly weighted evaluation using multiple models
>>>>>>> Stashed changes

   - **Environmental Built-in Libraries**: `sys`, `os`;
  
         * Libraries above should be pre-installed in a standard python environment *
   
  - **Third-party Processing Libraries**: `gensim`, `sklearn`, `numpy`, `nltk`, and `tqdm`;

        * Please refer to file "./requirements.txt" for automatic extension setup *

## 3.2 Replication Instruction
### 3.2.1 Command-based Execution 
(for New Models)
> or using execution commands...
>
(for Pre-trained Models)
> or using execution commands...
>

### 3.2.2 Script-based Execution 
(for New Models)
> a `Shell` program is prepared to process the execution...

(for Pre-trained Models)
> a `Shell` program is prepared to process the execution...



# Part 4 - Learning Designs

## 4.1 Model Construction
### 4.1.1 Machine Learning Models
- logistic reg
- knn
- svm
- nbs
- rocchio
- ### 4.1.2 Deep Learning Models
- RNN
- LSTM

## 4.2 Learning Patterns
- Cross validation
- train-valid-predict ratio
- Minibatch GD
- loop over hyperparamters

## 4.3 Storing and Reloading
### 4.3.1 Model Storing
### 4.3.2 Model Reloading
### 4.3.3 Model Re-training

## 4.4 Benchmarks and Analysis



# Part 5 - Performance Comparisons
## 5.1 Word2Vec-based Model Outcomes
|                        | [Logistic Regression]() | [k-Nearest Neighbors]() | [Rocchio]() | [Naive Bayes]() | [Support Vector Machine]() | 
|------------------------|-------------------------|-------------------------|-------------|-----------------|----------------------------|
| TP                     |                         |                         |             |                 |                            |                         
| FP                     |                         |                         |             |                 |                            |                         
| TN                     |                         |                         |             |                 |                            |                         
| FN                     |                         |                         |             |                 |                            |                         
| Accuracy               |                         |                         |             |                 |                            |                         
| F1_micro               |                         |                         |             |                 |                            |                         
| F1_macro               |                         |                         |             |                 |                            |                         
| Test Loss              |                         |                         |             |                 |                            |                         
| Optimal Hyperparameter |                         |                         |             |                 |                            |                         

|                        | [Fully-Connected NNs]() | [Recurrent NNs]() | [Long Short-term Memory NNs]() |
|------------------------|-------------------------|-------------------|--------------------------------|
| TP                     |                         |                   |                                |                    
| FP                     |                         |                   |                                |                        
| TN                     |                         |                   |                                |                           
| FN                     |                         |                   |                                |                      
| Accuracy               |                         |                   |                                |
| F1_micro               |                         |                   |                                |                   
| F1_macro               |                         |                   |                                |                     
| Test Loss              |                         |                   |                                |                          
| Optimal Hyperparameter |                         |                   |                                |                        

## 5.2 BoW-based Model Outcomes
|                        | [Logistic Regression]() | [k-Nearest Neighbors]() | [Rocchio]() | [Naive Bayes]() | [Support Vector Machine]() | 
|------------------------|-------------------------|-------------------------|-------------|-----------------|----------------------------|
| TP                     |                         |                         |             |                 |                            |                         
| FP                     |                         |                         |             |                 |                            |                         
| TN                     |                         |                         |             |                 |                            |                         
| FN                     |                         |                         |             |                 |                            |                         
| Accuracy               |                         |                         |             |                 |                            |                         
| F1_micro               |                         |                         |             |                 |                            |                         
| F1_macro               |                         |                         |             |                 |                            |                         
| Test Loss              |                         |                         |             |                 |                            |                         
| Optimal Hyperparameter |                         |                         |             |                 |                            |                         

|                        | [Fully-Connected NNs]() | [Recurrent NNs]() | [Long Short-term Memory NNs]() |
|------------------------|-------------------------|-------------------|--------------------------------|
| TP                     |                         |                   |                                |                    
| FP                     |                         |                   |                                |                        
| TN                     |                         |                   |                                |                           
| FN                     |                         |                   |                                |                      
| Accuracy               |                         |                   |                                |
| F1_micro               |                         |                   |                                |                   
| F1_macro               |                         |                   |                                |                     
| Test Loss              |                         |                   |                                |                          
| Optimal Hyperparameter |                         |                   |                                |                        


# Part 6 - Error Handling

# Part 7 - References

## 7.1 Reusing Claim
> * The primary parts of Training and Prediction in the `NBC model` are edited and reused;
> 
> * The model was taken from `NaiveBayesModel` constructed for `CMPUT 497/501: Intro to NLP` in Fall 22;
> 
> * Original authors who constructed the model: `Eden Zhou (Edennnnnnnnnn)` and `Menghan Chen (menghan-cmh)`, with original link to [GitHub Repo](https://github.com/UOFA-INTRO-NLP-F21/f2021-asn5-Edennnnnnnnnn);

## 7.2 External Reference
> About NLTK Methods:
> - https://www.nltk.org/



# ------------------------------------------------------------
# Program Architecture

## Model 01 - Naive Bayes (NBs) Classifier

### Algorithms & Logics
#### Training
> This implementation provides a framework for training a Laplace-smoothed (aka. add-one smoothing) Naive Bayes classifier on text data. The modelInput method preprocesses the training data by calling the dataFilter method to remove stopwords and punctuation marks from the text. 
> It then stores the category and preprocessed text in a hash table. The modelTrain method trains the Naive Bayes classifier model by counting the occurrences of words in each category and storing the prior probabilities and likelihoods of each word in a dictionary.

#### Prediction
> The Naive Bayes algorithm used in this implementation works by computing the probabilities of each category given a new input document, based on the prior probabilities and the likelihoods of each word in the test documents. The modelInput method preprocesses the training data by calling the dataFilter method to remove stopwords and punctuation marks. It assumes that each word in the document is independent of each other, which is why it is called "naive".
> The algorithm computes the log of the probabilities instead of the probabilities themselves to avoid numerical underflow issues that can occur when multiplying many small probabilities together. The log of the probabilities can be added instead of multiplied, which is more computationally efficient and avoids underflow issues.


### Execution Commands
#### Training
* To run `nbc_train.py` for training NBs models, using the command shown below:
 
  `python3 ./nbc/nbc_train.py [inputTrainingDataFilePath.json] [outputModelDataFilePath.tsv]`

  * The first bracket part in the bracket is related to a `.json` file that contains all training data; 
  * The second bracket defines the model output file, which should be a `.tsv` file;
 
* A Formal sample command that could be used under the program directory is:

  `python3 ./nbc/nbc_train.py ./data/train.json ./nbc_bbc_model.tsv`

#### Prediction
* To run `nbc_prediction.py` for prediction on NBs models, using the command shown below:
  
<<<<<<< Updated upstream
  `python3 ./nbc/nbc_prediction.py [outputModelDataFilePath.tsv] [inputTestDataFilePath.json]`
=======
  - **Interactive Options**:
    - `d` or `detect` - Checks risks in a **SQL injection input** using a **selected model**. 
      - A pop-up instruction will ask you to select a model, ignore it if no need;
    - `s` or `scan` - Scans a **SQL injection** with **all pre-trained models** for a comprehensive report. 
      - A weighted comprehensive risk score computed;
    - `f` or `file` - Scans a **log file** with **all pre-trained models** for comprehensive reports. 
      - A report table will be printed for line-by-line analysis;
    - `r` or `rebuild` - **Rebuilds a model** from scratch (Covers the pre-trained stored);
      - Follow the instructions to select the model type you want to rebuild;
      - The process will cover the pre-trained model stored and create a new learning diagram;
    - `h` or `help` - Displays the **help page**;
    - `e` or `exit` - **Exits** the program;
>>>>>>> Stashed changes

  * The first bracket part is related to a `.tsv` file that contains all model data learned; 
  * The second bracket part is related to a `.json` file that contains all test data; 

* A Formal sample command that could be used under the program directory is:

<<<<<<< Updated upstream
  `python3 ./nbc/nbc_prediction.py ./nbc_bbc_model.tsv ./data/test.json`

### Data Input
- By default, the program assumes the input training data files presented in the `/data` directory of the program package;
- And the original data should be stored in a file in the format of `.json`;
- Users may define their own input path (for training) by changing **the first bracket part** of the training command;
- Users may define their own input path (for test) by changing **the second bracket part** of the prediction command;

### Data Output
- Commonly, all the model data learned from model training will be stored in `.tsv` files in the program package;
  - Each TSV file contains data from an original JSON file, and it has **four** columns created: 
    - `type`, the type of the dataline, as "prior" or "likelihood";
    - `category`, the category of the dataline;
    - `priorProb/term`, the category prior probability if type="prior" / the term in string if type="likelihood";
    - `~/likelihoodProb`, None if type="prior" / the token (under spec-category) likelihood probability if type="likelihood";
- Users may define their own output path by changing **the second bracket part** of the training command;
- Commonly, the file created in training will be used for model test in prediction, defined by **the first bracket part** of the prediction command;

### Data Structures
- Raw data structure after JSON loading: a list contains multiple dictionaries, and each dictionary holds keys of doc, and text, each doc is considered as a data sample:
  - **Raw Input** >> List of dicts, \[{category_1:"XXX", text_1:"XX XX XXX""}, {category_2:"XXX", text_2:"XX XX XXX""}, ...]


- Rearranged training data structure: we assign each doc sample with and docID, this can be used as the key to take out the relevant data:
  - **Training/Test Data** >> Dict of nested-lists, {docID_1: \[docCategory_1, \[token_1_1, token_1_2, ...], docID_2:\[docCategory_2, \[token_2_1, token_2_2, ...]], ...}


- The model probability data structures for dara learned in training:
  - **Likelihoods Prob** >> Nested-dicts, {category_1: {token_1_1:99, token_1_2:88, ...}, category_2: {token_2_1:77, ...}, ...}
  - **Prior Prob** >> Hashtable, {category_1:0.999, category_2:0.1111, ...}


- The model predictions for each sample doc will be recorded as:
  - **Doc Prediction** >> Dict, {testDocId_1:"category", testDocId_2:"category", ...}

### Data Analysis
- The model prediction contains a method called self.modelAnalyze which will be call automatically to evaluate the model predictions;
- Analysis results needed will be printed to `STDOUT` channel:
  - Class-level: `true positive`, `false positive`, `false negative`, `true negative`;
  - Class-level: `precision`, `recall` and `F1 scores`;
  - Overall: `micro-averaged F1` & `macro-averaged F1` for the entire test set;
  
### Error Handling
- Any potential errors in given data or command processing will result in error messages (printed to `STDERR`) and running stop unexpectedly;


- `EmptyQueryError`: Input path(s) is/are invalid;
  - Invalid or empty path(s) input;
  - --> Error message to STDERR; Process Exits;


- `FilePathError`: The input file path is invalid; (Reminder: should end with '.json');
  - Reminder: the indexed file input should have a path/filename ends with ".json";
  - --> Error message to STDERR; Process Exits;


- `FilePathError`: The input file path is invalid; (Reminder: should end with '.tsv');
  - Reminder: the indexed file input should have a path/filename ends with ".tsv";
  - --> Error message to STDERR; Process Exits;


- `DetectionError`: Failed to detect input validation;
  - Satisfy the file format but cannot find the file within directory;
  - --> Error message to STDERR; Process Exits;


- `InputError`: Failed to input given data to the model; 
  - Cannot find the model file via input path;
  - --> Error message to STDERR; Process Exits;


- `ProcessingError`: Failed to process model training based on given data;
  - Failed to training model;
  - --> Error message to STDERR; Process Exits
=======
- **Replication and Reproduction**
  - For replication and reproduction purposes, execute `classifier.py` directly. "Classifier" is a development version which contains the whole process of **models building** and **hyperparameter find-tuning** from scratch;
  
        $ python3 Proj/classifier.py
  - This file will **automatically reprocess the whole learning task**, and **regenerate all information needed**: prediction outcomes, performance analysis data/diagrams, tuning/optimization reports, and pre-trained models;
    - ***WARNING***: The process may take hours to run. To speed up the Reproduction process, **avoid retraining the most time-consuming model `GBDT`** by **removing** it from the classifier list at [~Line 65] in `Classifier.py`;
  - Some arguments can be added to the command to fit the development demands:

        $ python3 classifier.py [k:<2~6>]
    - The **third part** of the command may accept a value that determines the **number of folds** in the cross-validation process. Where `k` indicates the **Fold Number** for the cross-validation process;

# Data Availability
## Data Input
- Storage: 
  - Refer to the directory of [`Proj/data`](https://github.com/CMPUT-416/Proj/blob/main/data), **four file pairs (four datasets)** can be found;
- Types:
  - Files with filenames end with `-sqli.txt` (e.g., burp-labels.txt) serve data **features**;
  - Files with filenames end with `-labels.txt` (e.g., burp-labels.txt) serve data **labels**;
  - Files for the same dataset should share the **same starts** of filenames. **Self-defined datasets** can be added following the same style;
- Formats:
  - For **sqli files**, each line in the file should contain one SQL injection semantics;
  - For **labels files**, each line in the file should contain one corresponding class label 
    - `1` for **high-risk vulnerable** samples; 
    - `0` for **low-risk non-vulnerable** samples;

## Data Preprocessing
- After reading SQL queries and the corresponding labels from files, tokenization, normalization and vectorization take over the texts, and then organize them into structured numerical vectors.
- Class `Preprocessor` from `preprocessor.py` preprocesses data before modelling, with steps:
  - **1. Tokenization**: 
    - Operation: Convert each SQL query into individual tokens, and remove spaces behind and in front of the expression; 
  - **2. Normalization**: 
    - Operation (Optional): Standardizing the raw text data for faster and more reasonable processing, `StandardScaler` from `sklearn` applied to scale the data;
  - **3. Vectorization**: 
    - Method: `Word2Vec` technique used for values embedding, which converts tokens into numerical vector representations;
    - Operation: `Doc2Vec` model (an improved version of Word2Vec) from `gensim` applied to process the **string-level vectorization**;
    - Dictionary Storage: The **vectorization model** is saved under the directory of [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), named `doc2vec.pkl`. Reuse when predicting;
  
         > ****Design and Concerns***: Although we have some models (e.g., Naive Bayes) that work well in processing string/token-based data, most of the most famous classification models are designed for numerical (vector-based) inputs. In this way, we selected the vectorization technique of  `Doc2Vec`. Vectorization is a key technique used to convert token expressions into numerical vector representations in Natural/Machine Language Processing. Some other methods like `Bag of Word (BoW)` may bring similar outcomes, but it may **suffer more from `Unknown (UNK)` words**, and it is **more costly** in saving the BoW model. The method of `Word2Vec` may also be applied in this case, but it has some limitations due to the **unbalanced length of input documents/queries** (must apply the second-level filter or take some information retrieval techniques such as `tf-idf` to re-balance the inputs) and maintains the limited relationship between tokens. By contrast, the approach of `Doc2Vec` provides **a more reasonable solution** for semantics with diverse sizes and it can provide a better solution in keeping *between-token dependencies* (which will facilitate the later model learning);



  - **4. Splitting**:
    - Method: Splitting into training and test sets, having data in sets ready for later model learning;
    - Operation: `train_test_split` method from `sklearn` applied to split the data input into two parts: 
    - Shuffle: Data **random shuffle** applied when various datasets are created;
      - Dataset for **Training & Validation (90%)**
      - Dataset for **Test (10%)**
  
         > ****Design and Concerns***: **Random data shuffle** helps improve model robustness to have more general performance but may also result in **unequal outcomes** for each time of modelling; The process will also **reorganize the data structure** to facilitate the processes of model learning;


# Model Learning
## Learning Design

  - **Model Methods**: 
    - Each model class contains methods for training, storing, reloading, and predicting;
    - A controller approach called `classify(datasets_train, datasets_test, doTraining: bool)` is established in each model to be called from the outers to process the model learning automatically. The model will execute
      - `Training + Prediction` when `doTraining=True`, OR 
      - only `Prediction` when `doTraining=False` with valida model reloaded;

  - **Hyperparameter Fine-tuning**: 
    - A systematic hyperparameter fine-tuning strategy is implemented. The optimal hyperparameter will be reported and applied to test directly;
    - The hyperparameter scope is designed by empiricism, and the selection is processed by loop-examinations;
    - Models with the optimal hyperparameter are trained and saved locally as the pre-trained models;
  
         > ****Design and Concerns***: The Fine-tuning process will examine models based on **different hyperparameters (loop over the empiricism-based hyperparameter selection scope)** under the same environment. The program will follow the same learning design in formal training by using cross-validation and the same pre-processing approaches. The optimal hyperparameter with the corresponding performance (e,g., F-scores) will **be reported**. The model with the best hyperparameter choice is **prepared and saved locally** for later prediction tasks.

  - **Cross-validation**: 
    - `3-fold cross-validation` applied, with **one sample model** built in each round. Data **random shuffle** is applied when folds of cross-validation are created.
    - Based on the outcomes from the previous tests, cross-validation with `3-folds` came with the optimal performance with a reasonable amount of data used for each round of training and validation;
    - For each round in cross-validation, a formal way of data splitting ratio works as `9:1` OR `6:3:1`, which means:
      - **60%** of all data used for the in-round **training**;
      - **30%** of all data used for the in-round **validation**;
      - **10%** of all data used for the final **test**;
    - The model with the best performance will be used in the **final test**, and stored locally as the **pre-trained model**;  
  
         > ****Design and Concerns***: The model with the best performance (the best **selected from in-fold models** based on the metric of `F-score`) with the optimal hyperparameter is **saved locally** for later prediction tasks. **Random shuffle** helps improve model robustness to reduce the probability of over-fitting but may result in **unequal outcomes** for each time of processing (users may **not expect to see the same outcomes** at each time of execution).

  - **Evaluation Metrics**:
    - `Mean Squared Error (MSE)`;
    - `F1-scores`: `Macro F1-score` AND `Micro F1-score` (core standard);
    - `balanced accuracy`;
  
         > ****Design and Concerns***: `Mean Squared Error (MSE)` is computed based on the difference between the **model predictions** and the **ground-truth labels**, which is mainly used to indicate model learning; The `F-score` is the **harmonic mean of `precision` and `recall`**, taking both `false positives` and `false negatives` into account. The **higher** the `F1-score` (i.e., `1` means taking **balanced consideration** between `precision` and `recall`), the **more accurate** the model is considered. Compared with other benchmarks, say `accuracy`, `F-scores` is a more reasonable standard (with **much less influence from the unbalanced data** and provides all-around analysis based on the `confusion matrix` (i.e., a table that helps visualize the performance of a classification model by **comparing the actual and predicted labels**)). **Both** benchmarks of `Micro F1-score`, and `Macro F1-score` are reported to have **more general consideration** regarding the performance. In-between, the core standard of `Micro F1-score` is directly used to **determine the optimal model**; `balanced accuracy` is an improved version of `accuracy` which is rebalanced and has less influence from the unbalanced learning data. This standard indicates the **correct prediction percentage** for each model.

  - **Diagram Outputs**: 
    - Diagrams Generation: visualization analysis outcomes are generated by method `_evaluate()` contained in each model during the building process and are stored in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval) to display the changes in model performance depending on:
      - Hyperparameter Choice vs. **Mean of MSE**;
      - Hyperparameter Choice vs. **Mean of F1-scores**;
      - Hyperparameter Choice vs. **Mean of accuracy**;
  
         > ****Design and Concerns***: **Selections of `Hyperparameter Choice`** are displayed on the `x-axis` of diagrams, and the outcomes of **different evaluation standards** corresponding to various hyperparameter choices are shown on the `y-axis`; Taking the `mean` from rounds of cross-validation for each hyperparameter and model to have **more general and precise evaluations**; A special case is that the model `GBDT` has **two hyperparameters for find-tuning**: more diagrams can be found in the directory to indicate the specific selections of the second hyperparameter.

  - **Saving and Reloading**: 
    - **Pre-trained Storages**: All machine learning models and a vectorization model prepared for this task are designed to be saved and reloaded when needed. In [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs) you may find
      - **Pre-trained Vectorization Dictionary Model**: `doc2vec.pkl`;
      - **Pre-trained Machine Learning Models**: `logistics.pkl`, `knn.pkl`, `nb.pkl`, `svm.pkl`, `dt.pkl`, and `gbdt.pkl`;
    - **Update Mechanism**:
      - *(Controlled Automatically)* Each time taking **data pre-processing** will train and save (**replace the exists**) a new `Vectorization Dictionary Model`, named `doc2vec.pkl`;
      - *(General Usage)* Each time calling `build` option in `detector.py` will train and save (**replace the exists**) a new specific `Machine Learning Model` based on the selection made;
      - *(Replication/Reproduction Usage)* Each time running `classifier.py` will redo the whole training and saving process, which means **all pre-trained models will be covered** with the new outcomes;


## Model Features
- **[Logistic Regression Model](https://github.com/CMPUT-416/Proj/blob/main/models/logReg.py)**
  
  > `Logistic Regression` is a statistical model that uses a logistic function to model a binary dependent variable, commonly used for predicting the probability of a certain class or event such as pass/fail;
  
  - Hyperparameter fine-tuning indicates that the **regularization strength** of `C=500` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `logistics.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[k-nearest Neighbors (k-NNs) Model](https://github.com/CMPUT-416/Proj/blob/main/models/knn.py)**
  
  > `k-Nearest Neighbors (k-NNs)` algorithm is a type of instance-based learning method used for classification and regression that predicts a data point's outcome based on the outcomes of its 'k' closest neighbours in the feature space.
  
  - Hyperparameter fine-tuning indicates that the **considered neighbor number** of `k=1` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `knn.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Naive Bayes (NBs) Model](https://github.com/CMPUT-416/Proj/blob/main/models/nb.py)**
  
  > `Naive Bayes (NBs)` is a generative model which means it predicts the joint probability, P(x, y), of the features, x, and the target, y, and it assumes that all the features are conditionally independent given the target.
  
  - Hyperparameter fine-tuning indicates that the **inverse regularization strength** of `α=1000` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `nb.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;
>>>>>>> Stashed changes


- `OutputError`: Failed to output the model data; 
  - Failed to print data learned by model; 
  - --> Error message to STDERR; Process Exits;
  
<<<<<<< Updated upstream

## Model 02 - Rocchio Classifier

### Algorithms & Logics
#### Training
> The model takes in a JSON file containing training data and tokenizes the text using NLTK's word_tokenize function. The text is then filtered to remove stop words and punctuation marks, and the resulting tokens are stored in a hash table along with their corresponding category label.
> The ltc-scheme is applied in computing the in-doc token weights, each category is finally described as a centroid. ALl token idf values are stored for prediction use;
#### Prediction
> The prediction part initializes a set of class variables to be used across the methods. It has methods to tokenize, pruning, set the input parameters, filter the data, load the input data (test data and model), and calculate the ltc-weights based on idf values stored for each token. 
> Euclidean Distance is used to check the spacial distance b/w the test doc and category centroids in the vector space;

### Execution Commands
#### Training
* To run `rocchio_train.py` for training Rocchio models, using the command shown below:
 
  `python3 ./rocchio/rocchio_train.py [inputTrainingDataFilePath.json] [outputModelDataFilePath.tsv]`

  * The first bracket part is related to a `.json` file that contains all training data; 
  * The second bracket defines the model output file, which should be a `.tsv` file;

* A Formal sample command that could be used under the program directory is:
 
  `python3 ./rocchio/rocchio_train.py ./data/train.json ./rocchio_bbc_model.tsv`

#### Prediction
* To run `rocchio_prediction.py` for prediction on Rocchio models, using the command shown below:
 
  `python3 ./rocchio/rocchio_prediction.py [outputModelDataFilePath.tsv] [inputTestDataFilePath.json]`

  * The first bracket part is related to a `.tsv` file that contains all model data learned; 
  * The second bracket part is related to a `.json` file that contains all test data; 

* A Formal sample command that could be used under the program directory is:

  `python3 ./rocchio/rocchio_prediction.py ./rocchio_bbc_model.tsv ./data/test.json`

### Data Input
- By default, the program assumes the input training data files presented in the `/data` directory of the program package;
- And the original data should be stored in a file in the format of `.json`;
- Users may define their own input path (for training) by changing **the first bracket part** of the training command;
- Users may define their own input path (for test) by changing **the second bracket part** of the prediction command;

### Data Output
- Commonly, all the model data learned from model training will be stored in `.tsv` files in the program package;
  - Each TSV file contains data from an original JSON file, and it has **three** columns created: 
    - `type`, the type of the dataline, as "centroid" or "idf";
    - `category/term`, the category type if type="centroid" / the term in string if type="idf";
    - `categoryCentroid/termIDF`, category centroid represent in string (for each token and value) if type="centroid" / the token idf value if type="idf";
- Users may define their own output path by changing **the second bracket part** of the training command;
- Commonly, the file created in training will be used for model test in prediction, defined by **the first bracket part** of the prediction command;

### Data Structures
- Raw data structure after JSON loading: a list contains multiple dictionaries, and each dictionary holds keys of doc, and text, each doc is considered as a data sample:
  - **Raw Input** >> List of dicts, \[{category_1:"XXX", text_1:"XX XX XXX""}, {category_2:"XXX", text_2:"XX XX XXX""}, ...]


- Rearranged training data structure: we assign each doc sample with and docID, this can be used as the key to take out the relevant data:
  - **Training/Test Data** >> Dict of nested-lists, {docID_1: \[docCategory_1, \[token_1_1, token_1_2, ...], docID_2:\[docCategory_2, \[token_2_1, token_2_2, ...]], ...}


- The model probability data structures for dara learned in training:
  - **Category Centroid** >> Nested-dicts, {category_1: {token_1_1:tokenWeight_1_1, token_1_2:tokenWeight_1_2, ...}, category_2: {token_2_1:tokenWeight_2_1, ...}, ...}
  - **Token IDF** >> Hashtable, {token_1:tokenIDF_1, token_2:0.tokenIDF_2, ...}


- The model predictions for each sample doc will be recorded as:
  - **Doc Prediction** >> Dict, {testDocId_1:"category", testDocId_2:"category", ...}
=======
- **[Support Vector Machine (SVM) Model](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py)**
  
  > `Support Vector Machine (SVM)` is a discriminative model which means it predicts the joint probability, P(x, y), of the features, x, and the target, y, and it assumes that all the features are conditionally independent given the target.
  
  - Hyperparameter fine-tuning indicates that the **intensity controller** of `C=100` brings the optimal model performance;
  - Pretrained model named `svm.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Decision Tree (DT) Model](https://github.com/CMPUT-416/Proj/blob/main/models/dt.py)**
  
  > `Decision Tree (DT)` is a classification model with a learning process that can be seen as finding an optimal tree structure so that the structure can well describe data characteristics. Each node of the tree corresponds to a feature, and the branching of the node corresponds to a decision made on that feature.
  
  - Hyperparameter fine-tuning indicates that the **tree searching depth** of `depth=7` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `dt.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create;
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;


- **[Gradient Boost Decision Tree (GBDT) Model](https://github.com/CMPUT-416/Proj/blob/main/models/gbdt.py)**
  
  > `Gradient Boosting Decision Tree (GBDT)` is a powerful machine learning algorithm that utilizes an ensemble of decision trees, where each tree is iteratively built to correct the mistakes of the previous trees, effectively improving prediction accuracy by minimizing a loss function via gradient descent.
  
  - Hyperparameter fine-tuning indicates that the **tree searching depth** of `depth=6` and **tree estimator num** of `n=100` brings the optimal model performance;
  - Evaluation metrics of `Mean Squared Error (MSE)`, `F1-scores`, and `balanced accuracy` used, where `F1-scores` is considered as the core indication;
  - Pretrained model named `gbdt.pkl` may be found in the directory [`Proj/srcs`](https://github.com/CMPUT-416/Proj/blob/main/srcs), otherwise using the **rebuild** option in the detector to create (may take minutes to process);
  - Evaluation Output of diagrams can be found in the directory [`Proj/eval`](https://github.com/CMPUT-416/Proj/blob/main/eval);
  - *All other model learning settings and patterns **obey the instruction shown in section `Learning Design`**;



## Model Performance
|                        | [Logistic Regression](https://github.com/CMPUT-416/Proj/blob/main/models/logReg.py) | [k-Nearest Neighbors](https://github.com/CMPUT-416/Proj/blob/main/models/knn.py) | [Naive Bayes](https://github.com/CMPUT-416/Proj/blob/main/models/nb.py) | [Support Vector Machine](https://github.com/CMPUT-416/Proj/blob/main/models/svm.py) | [Decision Tree](https://github.com/CMPUT-416/Proj/blob/main/models/dt.py) | [Gradient Boost Decision Tree](https://github.com/CMPUT-416/Proj/blob/main/models/gbdt.py) | 
|------------------------|-------------------------------------------------------------------------------------|----------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------|---------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|
| TP                     |                                                                                     |                                                                                  |                                                                         |                                                                                     |                                                                           |                                                                                            |                      
| FP                     |                                                                                     |                                                                                  |                                                                         |                                                                                     |                                                                           |                                                                                            |               
| TN                     |                                                                                     |                                                                                  |                                                                         |                                                                                     |                                                                           |                                                                                            | 
| FN                     |                                                                                     |                                                                                  |                                                                         |                                                                                     |                                                                           |                                                                                            |                    
| Accuracy               |                                                                                     |                                                                                  |                                                                         |                                                                                     |                                                                           |                                                                                            |                    
| F1_micro               |                                                                                     |                                                                                  |                                                                         |                                                                                     |                                                                           |                                                                                            |                       
| F1_macro               |                                                                                     |                                                                                  |                                                                         |                                                                                     |                                                                           |                                                                                            |                       
| Test Loss              |                                                                                     |                                                                                  |                                                                         |                                                                                     |                                                                           |                                                                                            |                       
| Optimal Hyperparameter | *C* = 500                                                                           | *k* = 1                                                                          | *α* = 1000                                                              | *C* = 100                                                                           | *depth* = 7                                                               | *depth* = 6   &   *n* = 100                                                                |                       

- ***Reminder***: Due to the data shuffle mechanism during the `data splitting` and `cross-validation`, the training and prediction outcomes may **vary** in different rounds of execution;



# Weighting and Detecting
>>>>>>> Stashed changes


- Distance recording and ranking data structures:
  - **Distance Recorder** >> Dict, {category_1:distToTestDocInInt_1, category_2:distToTestDocInInt_2, ...}
  - **Distance Ranker** >> MinHeap, \[(distToTestDocInInt_1, categoryCentroid_1), (distToTestDocInInt_2, categoryCentroid_2), ...]


### Data Analysis
- The model prediction contains a method called self.modelAnalyze which will be call automatically to evaluate the model predictions;
- Analysis results needed will be printed to `STDOUT` channel:
  - Class-level: `true positive`, `false positive`, `false negative`, `true negative`;
  - Class-level: `precision`, `recall` and `F1 scores`;
  - Overall: `micro-averaged F1` & `macro-averaged F1` for the entire test set;

### Error Handling
- `EmptyQueryError`: Input path(s) is/are invalid;
  - Invalid or empty path(s) input;
  - --> Error message to STDERR; Process Exits;


- `FilePathError`: The input file path is invalid; (Reminder: should end with '.json');
  - Reminder: the indexed file input should have a path/filename ends with ".json";
  - --> Error message to STDERR; Process Exits;


- `FilePathError`: The input file path is invalid; (Reminder: should end with '.tsv');
  - Reminder: the indexed file input should have a path/filename ends with ".tsv";
  - --> Error message to STDERR; Process Exits;


- `DetectionError`: Failed to detect input validation;
  - Satisfy the file format but cannot find the file within directory;
  - --> Error message to STDERR; Process Exits;


- `InputError`: Failed to input given data to the model; 
  - Cannot find the model file via input path;
  - --> Error message to STDERR; Process Exits;


- `ProcessingError`: Failed to process model training based on given data;
  - Failed to training model;
  - --> Error message to STDERR; Process Exits


- `OutputError`: Failed to output the model data; 
  - Failed to print data learned by model;
  - --> Error message to STDERR; Process Exits;


## Model 03 - k-Nearest Neighbors (kNN) Classifier

### Algorithms & Logics
#### Preparation
> The model takes in a JSON file containing training data and tokenizes the text using NLTK's word_tokenize function. The text is then filtered to remove stop words and punctuation marks, and the resulting tokens are stored in a hash table along with their corresponding doc label.
> The ltc-scheme is applied in computing the in-doc token weights, each doc is finally described as a vector. All token idf values are stored for prediction use;
#### Prediction
> The prediction part initializes a set of class variables to be used across the methods. It has methods to tokenize, pruning, set the input parameters, filter the data, load the input data (test data and model), and calculate the ltc-weights based on idf values stored for each token. 
> Euclidean Distance is used to check the spacial distance b/w the test doc and existed training doc vectors in the vector space;

### Execution Commands
#### Preparation
* To run `knn_create_vectors.py` for training kNN models, using the command shown below:

   `python3 ./knn/knn_create_vectors.py [inputTrainingDataFilePath.json] [outputModelDataFilePath.tsv]`

  * The first bracket part is related to a `.json` file that contains all training data; 
  * The second bracket defines the model output file, which should be a `.tsv` file;
 
* A Formal sample command that could be used under the program directory is:
  
  `python3 ./knn/knn_create_vectors.py ./data/train.json ./knn_bbc_vectors.tsv`

#### Prediction
* To run `knn_prediction.py` for prediction on kNN models, using the command shown below:
 
  `python3 ./knn/knn_prediction.py [outputModelDataFilePath.tsv] [inputTestDataFilePath.json] [hyperparameter:k-value]`

  * The first bracket part is related to a `.tsv` file that contains all model data learned; 
  * The second bracket part is related to a `.json` file that contains all test data; 
  * The third bracket part defines the hyperparameter (the k-value, determines how many neighbors will be considered) of the prediction model, which should be an `int` value;

* A Formal sample command that could be used under the program directory is:

  `python3 ./knn/knn_prediction.py ./knn_bbc_vectors.tsv ./data/test.json 11`

### Data Input
- By default, the program assumes the input training data files presented in the `/data` directory of the program package;
- And the original data should be stored in a file in the format of `.json`;
- Users may define their own input path (for training) by changing **the first bracket part** of the training command;
- Users may define their own input path (for test) by changing **the second bracket part** of the prediction command;
- Users may define their own hyperparameter (for test) by changing **the third bracket part** of the prediction command;

### Data Output
- Commonly, all the model data learned from model training will be stored in `.tsv` files in the program package;
  - Each TSV file contains data from an original JSON file, and it has **three** columns created: 
    - `type`, the type of the dataline, as "vector" or "idf";
    - `category/term`, the category type for doc if type="vector" / the term in string if type="idf";
    - `docVector/termIDF`, doc token values represent in string (for each token and value) if type="vector" / the token idf value if type="idf";
- Users may define their own output path by changing **the second bracket part** of the training command;
- Commonly, the file created in training will be used for model test in prediction, defined by **the first bracket part** of the prediction command;

### Data Structures
- Raw data structure after JSON loading: a list contains multiple dictionaries, and each dictionary holds keys of doc, and text, each doc is considered as a data sample:
  - **Raw Input** >> List of dicts, \[{category_1:"XXX", text_1:"XX XX XXX""}, {category_2:"XXX", text_2:"XX XX XXX""}, ...]


- Rearranged training data structure: we assign each doc sample with and docID, this can be used as the key to take out the relevant data:
  - **Training/Test Data** >> Dict of nested-lists, {docID_1: \[docCategory_1, \[token_1_1, token_1_2, ...], docID_2:\[docCategory_2, \[token_2_1, token_2_2, ...]], ...}


- The model probability data structures for dara learned in training:
  - **Doc Weights** >> Nested-dicts, {docID_1: {token_1_1:tokenWeight_1_1, token_1_2:tokenWeight_1_2, ...}, docID_2: {token_2_1:tokenWeight_2_1, ...}, ...}
  - **Token IDF** >> Hashtable, {token_1:tokenIDF_1, token_2:0.tokenIDF_2, ...}


- The model predictions for each sample doc will be recorded as:
  - **Doc Prediction** >> Dict, {testDocId_1:"category", testDocId_2:"category", ...}


- Distance recording and ranking data structures:
  - **Distance Recorder** >> Dict, {category_1:distToTestDocInInt_1, category_2:distToTestDocInInt_2, ...}
  - **Distance Ranker** >> MinHeap, \[(distToTestDocInInt_1, DocWeights_1), (distToTestDocInInt_2, DocWeights_2), ...]



### Data Analysis
- The model prediction contains a method called self.modelAnalyze which will be call automatically to evaluate the model predictions;
- Analysis results needed will be printed to `STDOUT` channel:
  - Class-level: `true positive (TP)`, `false positive (FP)`, `false negative (FN)`, `true negative (TN)`;
  - Class-level: `precision`, `recall` and `F1 scores`;
  - Overall: `micro-averaged F1` & `macro-averaged F1` for the entire test set;


### Error Handling
- `EmptyQueryError`: Input path(s) is/are invalid;
  - Invalid or empty path(s) input;
  - --> Error message to STDERR; Process Exits;


- `FilePathError`: The input file path is invalid; (Reminder: should end with '.json');
  - Reminder: the indexed file input should have a path/filename ends with ".json";
  - --> Error message to STDERR; Process Exits;


- `FilePathError`: The input file path is invalid; (Reminder: should end with '.tsv');
  - Reminder: the indexed file input should have a path/filename ends with ".tsv";
  - --> Error message to STDERR; Process Exits;


- `DetectionError`: Failed to detect input validation;
  - Satisfy the file format but cannot find the file within directory;
  - --> Error message to STDERR; Process Exits;


- `InputError`: Failed to input given data to the model; 
  - Cannot find the model file via input path;
  - --> Error message to STDERR; Process Exits;


- `ProcessingError`: Failed to process model training based on given data;
  - Failed to training model;
  - --> Error message to STDERR; Process Exits


- `OutputError`: Failed to output the model data; 
  - Failed to print data learned by model;
  - --> Error message to STDERR; Process Exits;



