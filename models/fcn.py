import os
import torch
import torch.optim as optim

from torch import nn
from sklearn.metrics import f1_score
from torch.autograd import Variable
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader



class FCNModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(FCNModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out

    def save(self, file_path='fcn_model.pth'):
        torch.save(self.state_dict(), file_path)

    def load(self, file_path='fcn_model.pth'):
        if os.path.isfile(file_path):
            self.load_state_dict(torch.load(file_path))
            self.eval()
        else:
            print(f"No saved model found at {file_path}.")

class CustomDataset(Dataset):
    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data

    def __getitem__(self, index):
        return self.X_data[index], self.y_data[index]

    def __len__ (self):
        return len(self.X_data)


class FCNClassifier:
    def __init__(self):
        self.train_loader = None
        self.test_loader = None

    def classify(self):
        # Load your data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        train_loader = DataLoader(dataset=CustomDataset(X_train, y_train), batch_size=32, shuffle=True)
        test_loader = DataLoader(dataset=CustomDataset(X_test, y_test), batch_size=1)

        model = FCNModel(input_size=X.shape[1], hidden_size=100, num_classes=len(set(y)))
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        for epoch in range(100):
            self._training(model, criterion, optimizer, train_loader)
        self._predict(model, test_loader)

    def _training(self, model, criterion, optimizer, train_loader):
        model.train()
        for X, y in train_loader:
            X = Variable(X)
            y = Variable(y)
            # Clear gradient buffers
            optimizer.zero_grad()
            # Get output given inputs
            outputs = model(X)
            # Get loss for the predicted output
            loss = criterion(outputs, y)
            # Get gradients w.r.t. parameters
            loss.backward()
            # Update parameters
            optimizer.step()


    def _predict(self, model, test_loader):
        model.eval()
        y_pred = []
        y_true = []
        with torch.no_grad():
            for X, y in test_loader:
                X = Variable(X)
                y = Variable(y)
                y_true.extend(y.tolist())
                # Forward pass only to get logits/output
                outputs = model(X)
                # Get predictions from the maximum value
                _, predicted = torch.max(outputs.data, 1)
                y_pred.extend(predicted.tolist())
        # Get F1 Score
        f1 = f1_score(y_true, y_pred, average='macro')
        print('F1 Score:', f1)
