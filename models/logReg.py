import joblib
import numpy as np

from matplotlib import pyplot as plt
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score


class LogRegModel:
    def __init__(self, cv: int, cvEval: tuple):
        """
           Taking a Logistic Regression model initialized; Set parameters for the model learning process;
        :param
            - cv, int, the fold number for cross-validation;
            - cvEval, tuple, contains all evaluation benchmarks selected that will be used for performance analysis;
        """
        # Training Records
        self.bestClassifier = None
        self.bestTrainingLoss = np.inf
        self.trainingLosses = []

        # Cross-validation Settings
        self.cvEval: tuple = cvEval
        self.cv: int = cv
        self.cvErrors: list = []
        self.cvFValues: list = []
        self.cvBalAcc:list = []

        # Hyperparameter Settings
        self.bestC = 0
        self.maxIter = 1000
        self.hyperParams = [0.001, 0.01, 0.05, 0.1, 1, 10, 50, 100, 500]

    def classify(self, datasets_train, datasets_test, doTraining=True):
        """
            This is a controller method that will automatically execute the model learning process. The parameter will
        determine the task of "training + prediction" (redoing the whole process) or "only prediction" (after reloading);
        :param
            - datasets_train, tuple, training datasets for the model;
            - datasets_test, tuple, testing datasets for the model;
            - doTraining, bool, whether to perform a training phase;
        """
        if doTraining:
            # Take the training process
            self._training(datasets_train=datasets_train)
            self._evaluate()
            self._store()
        # Take the prediction process
        self._predict(datasets_test=datasets_test, doTraining=doTraining)

    def _training(self, datasets_train: tuple):
        """
            This method is used to train the Logistic Regression model by taking cross-validations for each selection of the
        hyperparameters; The method will figure out the optimal choice by checking the F1-score;
        :param
            - datasets_train, tuple, training datasets for the model;
        """
        # Unpacking X (features), y (labels) from the training dataset
        X_train = datasets_train[0]
        y_train = np.ravel(datasets_train[1])

        # Loop over each selection of the hyperparameter to process the learning
        for hyperParam in self.hyperParams:
            # Taking cross-Validations and acquiring model performance indications based on MSE, F-score and accuracy
            MSE = -cross_val_score(LogisticRegression(C=hyperParam, max_iter=self.maxIter),
                                   X=X_train,
                                   y=y_train,
                                   scoring=self.cvEval[0],
                                   cv=self.cv,
                                   n_jobs=-1)
            F = cross_val_score(LogisticRegression(C=hyperParam, max_iter=self.maxIter),
                                X=X_train,
                                y=y_train,
                                scoring=self.cvEval[1],
                                cv=self.cv,
                                n_jobs=-1)
            balancedAccuracies = cross_val_score(LogisticRegression(C=hyperParam, max_iter=self.maxIter),
                                                 X=X_train,
                                                 y=y_train,
                                                 scoring=self.cvEval[2],
                                                 cv=self.cv,
                                                 n_jobs=-1)
            # Record the performance for a specific hyperparameter, taking the mean performance over folds of cross-validation
            self.cvErrors.append(np.mean(MSE))
            self.cvFValues.append(np.mean(F))
            self.cvBalAcc.append(np.mean(balancedAccuracies))

        # Apply the best hyperparameter to build a formal classifier
        self.bestC = self.hyperParams[self.cvFValues.index(max(self.cvFValues))]
        self.bestClassifier = LogisticRegression(C=self.bestC, max_iter=self.maxIter)
        self.bestClassifier.fit(X_train, y_train)

    def _store(self):
        """
            This method is used to store the best classifier model into a local file;
        """
        # Save the best sample model with the optimal hyperparameter to the directory `Proj/srcs` with name `dt.pkl`
        joblib.dump(self.bestClassifier, 'srcs/logistics.pkl')

    def _reload(self):
        """
            This method is used to reload the pre-trained optimal classifier model from a local file;
        """
        # Reload the best sample model with the optimal hyperparameter from the directory `Proj/srcs` with name `dt.pkl`
        self.bestClassifier = joblib.load('srcs/logistics.pkl')

    def _predict(self, datasets_test: tuple, doTraining: bool):
        """
            This method is used to predict the testing datasets and evaluate its performance;
        :param
            - datasets_test, tuple, testing datasets for the model;
            - doTraining, bool, whether the model has been trained before prediction, for different message indications;
        """
        # Unpack datasets and then feed into the model
        X_test = datasets_test[0]
        y_test = np.ravel(datasets_test[1])

        # Compare predictions with the ground truth values to compute the accuracy and macro-F1-score
        y_test_hat = self.bestClassifier.predict(X_test)
        testFScoreMacro = f1_score(y_test, y_test_hat, average='macro')
        testFScoreMicro = f1_score(y_test, y_test_hat, average='micro')
        accuracy = accuracy_score(y_test, y_test_hat)

        results = (f"\n> Model of Logistic Regression:"
                f"\n\t* Optimal Hyperparameter --> C={self.bestC}"
                f"\n\t* Test Accuracy --> Accuracy={accuracy}"
                f"\n\t* Test F-score (Macro) --> F1 (Macro)={testFScoreMacro}"
                f"\n\t* Test F-score (Micro) --> F1 (Micro)={testFScoreMicro}\n")

        # Outputs indication information
        if doTraining:
            filename = "eval/LogRegModel_results_training.txt"
            print(results)
        else:
            filename = "eval/LogRegModel_results_pretrained.txt"

        with open(filename, "w") as file:
            file.write(results)

        print(f"Results saved to {filename}")



    def simpleDetect(self, sql):
        """
            A simple version for prediction designed for our quick-check detector; Performs a prediction for a
        single-line input;
        :param
            - sql, numpy.ndarray, the input data to predict;
        :return
            - predictions, numpy.ndarray, the prediction result for the input of SQL semantics.
        """
        # Resize the input data and ask the model to make predictions
        return self.bestClassifier.predict(sql.reshape(1, -1))

    def _evaluate(self):
        """
            Inner method used to evaluate the performance of the model by plotting analysis diagrams of hyperparameter
        versus mean accuracy, average mean squared error, and F1-Values;
        """
        # Create and save an analysis diagram about "Hyperparameter vs. Balanced Accuracy" to `Proj/eval`
        plt.figure()
        plt.plot(self.hyperParams, self.cvBalAcc, color='red')
        plt.xlabel('C-Value (HyperParam)')
        plt.ylabel('Mean of Balanced Accuracy (in Cross Validation)')
        plt.title('C-Value (HyperParam) vs. Mean of Balanced Accuracy')
        plt.savefig("eval/LogRegModel_1_hyperParams_vs_balanced_accuracy.png")

        # Create and save an analysis diagram about "Hyperparameter vs. Mean Squared Error" to `Proj/eval`
        plt.figure()
        plt.plot(self.hyperParams, self.cvErrors, color='green')
        plt.xlabel('C-Value (HyperParam)')
        plt.ylabel('Average MSE (in Cross Validation)')
        plt.title('C-Value (HyperParam) vs. Average MSE')
        plt.savefig("eval/LogRegModel_2_hyperParams_vs_cvAvgErrors.png")

        # Create and save an analysis diagram about "Hyperparameter vs. F1-values" to `Proj/eval`
        plt.figure()
        plt.plot(self.hyperParams, self.cvFValues, color='orange')
        plt.xlabel('C-Value (HyperParam)')
        plt.ylabel('Mean of  F1-values (in Cross Validation)')
        plt.title('C-Value (HyperParam) vs. Mean of F1-Values')
        plt.savefig("eval/LogRegModel_3_hyperParams_vs_cvFValues.png")
