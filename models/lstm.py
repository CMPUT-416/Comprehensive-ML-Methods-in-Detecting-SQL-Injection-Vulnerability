import numpy as np
import torch
import torch.nn as nn

from torch.utils.data import TensorDataset, DataLoader


class LSTMClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1):
        super(LSTMClassifier, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # LSTM Layer
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        # Fully connected layer
        self.fc = nn.Linear(hidden_dim, output_dim)
        # Sigmoid layer
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x, _ = self.lstm(x)
        s, b, h = x.shape
        x = x.view(s * b, h)
        x = self.fc(x)
        x = x.view(s, b, -1)
        """
        # Initialize hidden state with zeros
        h0 = torch.zeros(1, self.hidden_dim).to(x.device)
        # Initialize cell state
        c0 = torch.zeros(1, self.hidden_dim).to(x.device)
        # Pass the input tensor through LSTM
        out, _ = self.lstm(x, (h0, c0))
        # Pass the output tensor through the fully connected layer
        out = self.fc(out[:, -1, :])
        # Pass the output tensor through the sigmoid activation function
        out = self.sigmoid(out)
        return out"""
        return x


class LSTMModel:
    def __init__(self, cv: int, cvEval: tuple):
        # Training Settings:
        self.model = LSTMClassifier(input_dim=100, hidden_dim=4, output_dim=1, num_layers=2)
        self.criterion = nn.BCELoss()
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        self.bestTrainingLoss = torch.inf
        self.trainingLosses = []

        self.epochNum = 25
        self.batchSize = 64

        # Cross Validation Settings:
        self.cvEval: tuple = cvEval
        self.cv: int = cv
        self.cvErrors: list = []
        self.cvFValues: list = []
        self.cvBalAcc:list = []

    def training(self, datasets_train: tuple):
        # Data Unpacking:
        features = torch.tensor(datasets_train[0]).float().reshape(-1, self.batchSize, 100)
        labels = torch.tensor(np.ravel(datasets_train[1])).float().reshape(-1, self.batchSize, 100)
        n=999
        feature1s = torch.randn((n, 100))  # replace with your data
        label1s = torch.randint(0, 2, (n,))  # replace with your data
        dataset = TensorDataset(features, labels)
        data_loader = DataLoader(dataset, batch_size=self.batchSize, shuffle=True)
        for epoch in range(self.epochNum):
            for i, (features, labels) in enumerate(data_loader):
                # Forward pass
                outputs = self.model(features)
                loss = self.criterion(outputs, labels.float().unsqueeze(1))

                # Backward and optimize
                self.optimizer.zero_grad()
                self.trainingLosses.append(loss)
                loss.backward()
                self.optimizer.step()

                # Print the loss for every 100 iterations
                if (i + 1) % 100 == 0:
                    print(
                        f'Epoch [{epoch + 1}/{self.epochNum}], Step [{i + 1}], Loss: {loss.item():.4f}')

        self.bestTrainingLoss = min(self.trainingLosses)
        print(self.bestTrainingLoss)

    def test(self, datasets_test: tuple):
        pass
        # Data Unpacking:
        """
        X_test = datasets_test[0]
        y_test = np.ravel(datasets_test[1])
        #X_test = self.scaler.transform(X_test)

        # Classifier Testing:
        y_test_hat = self.bestClassifier.predict(X_test)
        testFScore = f1_score(y_test, y_test_hat)

        print(f"\n\t> Model of Naive Bayes (NBs):"
              f"\n\t\t*   Optimal Hyperparameters "
              f"\n\t\t        --> α={self.bestAlpha}"
              f"\n\t\t*   finalTestFScore "
              f"\n\t\t        --> F1={testFScore}")"""

    def evaluate(self):
        pass
        """
        plt.figure()
        plt.plot(self.hyperParams, self.cvBalAcc, color='red')
        plt.xlabel('α-Value (HyperParam)')
        plt.ylabel('Mean of Balanced Accuracy (in Cross Validation)')
        plt.title('α-Value (HyperParam) vs. Mean of Balanced Accuracy')
        plt.savefig("eval/NBModel_1_hyperParams_vs_balanced_accuracy.jpg")

        plt.figure()
        plt.plot(self.hyperParams, self.cvErrors, color='green')
        plt.xlabel('α-Value (HyperParam)')
        plt.ylabel('Average MSE (in Cross Validation)')
        plt.title('α-Value (HyperParam) vs. Average MSE')
        plt.savefig("eval/NBModel_2_hyperParams_vs_cvAvgErrors.png")

        plt.figure()
        plt.plot(self.hyperParams, self.cvFValues, color='orange')
        plt.xlabel('α-Value (HyperParam)')
        plt.ylabel('Mean of F1-values (in Cross Validation)')
        plt.title('α-Value (HyperParam) vs. Mean of F1-Values')
        plt.savefig("eval/NBModel_3_hyperParams_vs_cvFValues.png")"""

