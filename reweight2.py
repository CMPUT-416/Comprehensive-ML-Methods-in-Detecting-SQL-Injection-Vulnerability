import xgboost as xgb
import numpy as np
from sklearn.model_selection import KFold


# Read in traing data from files
y, ybar = [], []
with open("weights/ybar.txt", 'r') as file:
    # Use eval to convert string to list
    for line in file:
        ybar.append(eval(line.strip()))
with open("weights/y.txt", 'r') as file:
    # Remove newline character and save label
    for line in file:
        y.append(line.strip())

# Reorganize data structure
ybar = np.array(ybar).astype(np.int64)
y = np.array(y).astype(np.int64)
data_dmatrix = xgb.DMatrix(data=ybar,label=y)

# List of alpha values to try
alphas = [0, 0.001, 0.01, 0.1, 1, 10, 100]

# Initialize dictionary to store CV results
cv_results = {}
kf = KFold(n_splits=3, shuffle=True, random_state=42)
for alpha in alphas:
    params = {
        'objective': 'reg:squarederror',
        'max_depth': 10,
        'alpha': alpha
    }
    # Store CV results
    cv_result = xgb.cv(params, data_dmatrix, num_boost_round=50, folds=kf, metrics='rmse', early_stopping_rounds=10, seed=42)
    cv_results[alpha] = cv_result

# Print CV results
for alpha, result in cv_results.items():
    print(f"alpha: {alpha}, best RMSE: {result['test-rmse-mean'].min()}")

# Select the best alpha
best_alpha = min(cv_results, key=lambda alpha: cv_results[alpha]['test-rmse-mean'].min())
print(f"Best alpha: {best_alpha}")
params = {
    'objective': 'reg:squarederror',
    'max_depth': 10,
    'alpha': best_alpha
}

# Take model training and report the weight results
model = xgb.train(params, data_dmatrix)
print("MODEL WEIGHTS: ", model.get_score(importance_type="weight"))
weights = model.get_score(importance_type="weight").values()
SUM = sum(weights)
weights = [weight / SUM * len(weights) for weight in weights]
print(weights)