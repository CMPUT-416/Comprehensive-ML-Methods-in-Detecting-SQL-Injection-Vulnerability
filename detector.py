import os
import sys
import numpy as np

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from nltk import word_tokenize
from tqdm import tqdm
from torch import Tensor

from models.knn import KNNModel
from models.nb import NBModel
from models.logReg import LogRegModel
from models.svm import SVMModel
from models.baseline import BaselineModel
from models.lstm import LSTMModel
from models.decision_tree import DecisionTreeModel
from models.decision_tree import DecisionTreeModel
from models.gbdt import GBDTModel


# Global Variable Settings
SELECTIONS = {
    1: "Logistic Regression",
    2: "Naive Bayes",
    3: "k-nearest Neighbor",
    4: "Support Vector Machine",
    5: "Decision Tree",
    6: "Gradient Boost Decision Tree"
}
CV = 3
EVAL = ('neg_mean_squared_error', 'f1_macro', 'balanced_accuracy')


# Color Highlight Settings
B = "\033[0;34;40m"     # BLUE
Y = "\033[0;33;40m"     # YELLOW
G = "\033[0;32;40m"     # GREEN
R = "\033[0;31;40m"     # RED
RESET = "\033[0m"       # COLOR RESET


class Classifier:
    def __init__(self, cv: int, preprocessor, classifiers: list, evals: tuple, MODE):
        self.preprocessor = preprocessor
        self.cv: int = cv
        self.evals: tuple = evals
        self.classifiers: list = classifiers
        self.MODE = MODE

    def classify(self):
        print("+" * 60)
        print("\t\t\t\t** MODEL LEARNING **")
        print("+" * 60 + '\n')

        print(f">> Training Settings"
              f"\n\t> Method of {self.cv}-fold Cross-validation"
              f"\n\t> Benchmarks of {self.evals}")

        print(f">> Modelling & Predictions")
        for classifier in tqdm(self.classifiers):
            print('\n')
            if self.MODE == "PRE":
                classifier.classify(self.preprocessor.datasets_train, self.preprocessor.datasets_test, False)
class Interactor:
    def __init__(self):
        """
            Initializes the interactor with an SVM model and a pre-trained embedding;
        """
        # Load the default setting to start
        print("\n>> Initializing detector...")
        self.SELECT = 4
        self.detector = SVMModel(cv=CV, cvEval=EVAL)
        self.detector._reload()
        self.embedder = Doc2Vec.load("srcs/doc2vec.pkl")
        print(">> DONE.\n")

    def help(self, init=False):
        """
            This method is used to print out the help instructions and introduce the functions of the detector;
        """
        # Outputs indication information
        if init:
            print(">> Hi, this detector applies multiple ML models to check risks of SQL semantic inputs, you could")
        print(Y + "* Enter <(r)ebuild> to rebuild a model from scratch (cover the pre-trained)")
        print("* Enter <(d)etect> to check a SQL input by a specific pretrained detector")
        print("* Enter <(s)can> to scan a SQL input with all pretrained detectors")
        print("* Enter <(f)ile> to scan a log file with all pretrained detectors")
        print("* Enter <(h)elp> to review the instruction page")
        print("* Enter <(e)xit> to exit")
        print(RESET)

    def detect(self):
        """
            This method is used to detect the  SQL injection vulnerability in an input text by selecting a specific
        pretrained detector;
        """
        # Ask for a model selection
        self.select()

        # Outputs indication information to ask for an input of SQL semantics
        print(">> Enter your SQL semantics to test")
        sql = input("> ")
        # Preprocess the input and the call the .simpleDetect() to detect it
        sql_tokens = nltk.word_tokenize(sql.strip())
        sql_vector = self.embedder.infer_vector(sql_tokens)
        outcome = self.detector.simpleDetect(sql_vector)

        # Outputs indication information about the detection outcomes about the risk detected
        if outcome[0] == 0:
            risk = "HIGH SQL INJECTION RISK"
        else:
            risk = "LOW SQL INJECTION RISK"
        print(f"\n>> Detection result: {risk} \n(predicted by Model {self.SELECT} - {SELECTIONS[self.SELECT]})\n")

    def select(self):
        """
            This method is used to select a specific classification model for detecting the SQL injection vulnerability;
        """
        # Outputs indication information
        print(f">> Current loaded detector: Model {self.SELECT} - {SELECTIONS[self.SELECT]}")
        print(">> Enter an ID to change detector, you may have"
              "\n\t* 1=LogisticRegression;"
              "\n\t* 2=Naive Bayes;"
              "\n\t* 3=k-nearest Neighbors;"
              "\n\t* 4=Support Vector Machine;"
              "\n\t* 5=Decision Tree;"
              "\n\t* 6=Gradient Boost Decision Tree;")
        try:
            # Outputs indication information to ask for a specific selection of classification models
            self.SELECT = int(input("> "))
            if self.SELECT == 0:
                pass
            elif self.SELECT == 1:
                self.detector = LogRegModel(cv=CV, cvEval=EVAL)
            elif self.SELECT == 2:
                self.detector = NBModel(cv=CV, cvEval=EVAL)
            elif self.SELECT == 3:
                self.detector = KNNModel(cv=CV, cvEval=EVAL)
            elif self.SELECT == 4:
                self.detector = SVMModel(cv=CV, cvEval=EVAL)
            elif self.SELECT == 5:
                self.detector = DTModel(cv=CV, cvEval=EVAL)
            elif self.SELECT == 6:
                self.detector = GBDTModel(cv=CV, cvEval=EVAL)
            else:
                classifier.classify(self.preprocessor.datasets_train, self.preprocessor.datasets_test, True)

        print('\n' + "+" * 60)
        print("\t\t\t** MODEL PREDICTION ACCOMPLISHED **")
        print("+" * 60 + '\n\n')


class Preprocessor:
    def __init__(self, dirPath="data/", d2vModel=None):
        self.d2vModel = d2vModel
        self.datasets_train = {}
        self.datasets_test = {}
        self.dirPath = dirPath

    def process(self):
        print("+" * 60)
        print("\t\t\t\t** DATA PREPROCESSING **")
        print("+" * 60 + '\n')

        self._dataVectorization()
        data: tuple = self._dataInput()
        self._dataSplitting(data, ratio=0.1)
        #self._dataScaling()

        print('\n' + "+" * 60)
        print("\t\t\t** DATA PREPROCESS ACCOMPLISHED **")
        print("+" * 60 + '\n\n')

    def _dataNormalization(self, doc):
        return word_tokenize(doc.strip())

    def _dataVectorization(self, vector_size=100, window=5, min_count=1, workers=4):
        print(f">> Data Vectorization"
              f"\n\t> Method of Word2Vec/Doc2Vec")
        documents = []
        for filename in os.listdir(self.dirPath):
            if filename[-8:] == 'sqli.txt':
                print(f"\t\t* Converting dataset - {filename}...")
                filePath = os.path.join(self.dirPath, filename)
                with open(filePath, 'r') as f:
                    for index, dataline in enumerate(f):
                        tokens = self._dataNormalization(dataline)
                        documents.append(TaggedDocument(words=tokens, tags=[index]))
        self.d2vModel = Doc2Vec(documents, vector_size=vector_size, window=window, min_count=min_count, workers=workers)
        self.d2vModel.save("srcs/doc2vec.pkl")

    def _dataInput(self):
        print(f">> Data Input")
        features_hashtable = []
        labels_hashtable = []

        files = os.listdir(self.dirPath)
        txt_files = [f for f in files if f.endswith('.txt')]
        assert len(txt_files) % 2 == 0
        txt_files.sort()

        for i in range(0, len(txt_files), 2):
            f1 = os.path.join(self.dirPath, txt_files[i])
            f2 = os.path.join(self.dirPath, txt_files[i + 1])
            if f1.split("-")[1] == "sqli.txt":
                feature_file = f1
                label_file = f2
                raise
        except:
            # If the input is invalid, then report the issue and redo a selection
            sys.stderr.write("\n>> ModelSelectionError: Invalid model selection.")
            self.select()
            return
        # Reload the model depends on the user selection and give feedback
        self.detector._reload()
        print(">> Model reload successfully")

    def rebuild(self):
        """
            This method is used to rebuild a specific machine learning model based on the demand from training;
        """
        while True:
            # Output indication information to confirm the rebuilding demand
            print(f">> Current loaded detector: Model {self.SELECT} - {SELECTIONS[self.SELECT]}")
            IN = input(">> Do you want to rebuild it? [(y)es for rebuilding] [(s)elect for switching model] \n> ").strip().lower()
            if IN == "yes" or IN == "y":
                print(f"* WARNING: This operation will cover the existing pre-training one;")
                IN2 = input(f">> Are you sure you want to rebuild Model {self.SELECT} - {SELECTIONS[self.SELECT]}? [(y)es to continue] [(n)o to cancel] \n> ")
                if IN2 == 'no' or IN2 == "n":
                    break

                # Start the rebuilder, first take data reprocess and then retrain a specific model on it
                print(f"\n>> Data Preprocessor Ready.")
                P = Preprocessor(dirPath="data/")
                P.process()
                print(f"\n>> Model Trainer Ready.")
                if self.SELECT == 1:
                    self.detector = LogRegModel(cv=CV, cvEval=EVAL)
                    self.detector.classify(P.datasets_train, P.datasets_test, True)
                elif self.SELECT == 2:
                    self.detector = NBModel(cv=CV, cvEval=EVAL)
                    self.detector.classify(P.datasets_train, P.datasets_test, True)
                elif self.SELECT == 3:
                    self.detector = KNNModel(cv=CV, cvEval=EVAL)
                    self.detector.classify(P.datasets_train, P.datasets_test, True)
                elif self.SELECT == 4:
                    self.detector = SVMModel(cv=CV, cvEval=EVAL)
                    self.detector.classify(P.datasets_train, P.datasets_test, True)
                elif self.SELECT == 5:
                    self.detector = DTModel(cv=CV, cvEval=EVAL)
                    self.detector.classify(P.datasets_train, P.datasets_test, True)
                elif self.SELECT == 6:
                    self.detector = GBDTModel(cv=CV, cvEval=EVAL)
                    self.detector.classify(P.datasets_train, P.datasets_test, True)

                # Output indication information to show the rebuilding result
                print(f"\n>> Model Rebuilt Successfully.")
                print(f">> Current loaded detector: Model {self.SELECT} - {SELECTIONS[self.SELECT]}")
                break

            # Switch to other model selection if required
            elif IN == "select" or IN == "s":
                self.select()
            # For any other invalid input, stop the rebuilding process
            else:
                feature_file = f2
                label_file = f1

            with open(feature_file, 'r') as fF, open(label_file, 'r') as fL:
                for index, (feature, label) in enumerate(zip(fF, fL)):
                    tokens = self._dataNormalization(feature)
                    vector = self.d2vModel.infer_vector(tokens)
                    features_hashtable.append(vector)
                    labels_hashtable.append(int(label.strip()))
        return np.array(features_hashtable), np.array(labels_hashtable)

    def _dataSplitting(self, datasets: tuple, ratio=0.1):
        print(f">> Data Splitting")
        X_train, X_test, y_train, y_test = train_test_split(datasets[0], datasets[1], test_size=ratio, shuffle=True)
        self.datasets_train = [X_train.astype(float), y_train.astype(int)]
        self.datasets_test = [X_test.astype(float), y_test.astype(int)]
        print(f"\t> Data Loading:"
              f"\n\t      * datasets_train"
              f"\n\t\t        --> X: {self.datasets_train[0].shape}, y: {self.datasets_train[1].shape}"
              f"\n\t      * datasets_test"
              f"\n\t\t        --> X: {self.datasets_test[0].shape}, y: {self.datasets_test[1].shape}")

    def _dataScaling(self):
        print(f">> Data Scaling")
        scaler = StandardScaler().fit(self.datasets_train[0])
        self.datasets_train[0] = scaler.transform(self.datasets_train[0])
        self.datasets_test[0] = scaler.transform(self.datasets_test[0])



def main(argv):
    """ Main Func """
    # Possible cmd:
    #       python detector.py [MODE:<NEW/PRE>] [SELECTION:<*/0~8>] [CV:<2~5>]
    # 0=Baseline;
    # 1=LogisticRegression;
    # 2=Naive Bayes;
    # 3=k-nearest Neighbors;
    # 4=Support Vector Machine;
    # 5=FCN;
    # 6=RNN/LSTM;



    """ Dashboard """
    MODE = "NEW"
    SELECTION = '*'
    CV = 3
    evals = ('neg_mean_squared_error', 'f1_micro', 'balanced_accuracy')
    """ Dashboard """



    try:
        if argv[2] is not None:
            MODE = argv[1]
            assert MODE == "NEW" or MODE == "PRE"
        if argv[2] is not None:
            SELECTION = argv[2]
        if argv[3] is not None:
            CV = int(argv[3])
                break

    def scan(self, sql=None, info=False):
        """
            This method is used to scan an input with all pre-trained detectors and provides a weighted risk assessment;
        :param
            - sql, str, an input SQL semantics waiting for scanning;
            - info, bool, provides indication for the message printing;
        :return
            - (meanEval, risk), tuple, the scanning outcomes about the risk index and the risk extent;
        """
        # Prepare all classifier and the corresponding weights to process the all-around detection
        detectors = [LogRegModel(cv=CV, cvEval=EVAL),
                     NBModel(cv=CV, cvEval=EVAL),
                     KNNModel(cv=CV, cvEval=EVAL),
                     SVMModel(cv=CV, cvEval=EVAL),
                     DTModel(cv=CV, cvEval=EVAL),
                     GBDTModel(cv=CV, cvEval=EVAL)]
        weights = [1.25, 0.5, 1.25, 1.5, 0.5, 1]
        # Reload each models
        for detector in detectors:
            detector._reload()

        # Ask for an input of SQL semantics to process the scanning
        if sql is None:
            info = True
            print(">> Ready for joint scanning.")
            print(">> Enter the injected SQL semantics to evaluate")
            sql = str(input("> ").strip())
        # Preprocess the input of SQL semantics
        sql_tokens = nltk.word_tokenize(sql.strip())
        sql_vector = self.embedder.infer_vector(sql_tokens)

        # Take account of the weights for the prediction of models and compute an overall risk score, take an average to make sure of the result generalization;
        meanEvals = []
        epochNum = 3
        for i in range(epochNum):
            outcomes = []
            for detector in detectors:
                outcomes.append(detector.simpleDetect(sql_vector) * weights[detectors.index(detector)])
            meanEvals.append(sum(outcomes) / len(outcomes))
        meanEval = sum(meanEvals) / epochNum

        # Based on the weighted scanning report, mark the risk level of the input
        if meanEval >= 0.9:
            risk = "VERY HIGH RISK"
            print(R)
        elif meanEval >= 0.7:
            risk = "HIGH RISK"
            print(Y)
        elif meanEval >= 0.6:
            risk = "MEDIUM RISK"
            print(Y)
        elif meanEval >= 0.4:
            risk = "LOW RISK"
            print(G)
        else:
            raise
    except:
        MODE = MODE
        SELECTION =  SELECTION
        CV = CV
        #sys.stderr.write("\n>> CommandError: Invalid command entered, please check the command and retry;")
        #exit()

    # Initialize new models:
    classifiers = []
    if SELECTION == '*':
        classifiers = [LogRegModel(cv=CV, cvEval=evals),
                       NBModel(cv=CV, cvEval=evals),
                       KNNModel(cv=CV, cvEval=evals),
                       SVMModel(cv=CV, cvEval=evals),
                       DecisionTreeModel(cv=CV, cvEval=evals)]
    elif SELECTION == 0:
        pass
    elif SELECTION == 1:
        classifiers.append(LogRegModel(cv=CV, cvEval=evals))
    elif SELECTION == 2:
        classifiers.append(NBModel(cv=CV, cvEval=evals))
    elif SELECTION == 3:
        classifiers.append(KNNModel(cv=CV, cvEval=evals))
    elif SELECTION == 4:
        classifiers.append(SVMModel(cv=CV, cvEval=evals))
    elif SELECTION == 5:
        pass
    elif SELECTION == 6:
        classifiers.append(DecisionTreeModel(cv=CV, cvEval=evals))
    else:
        sys.stderr.write("\n>> ModelSelectionError: Invalid model selection, valid scope is [0-6];")
        exit()

    # Apply pre-trained models if needed:
    if MODE == "NEW":
        for classifier in classifiers:
            classifier._reload()
    print(f"\n>> Execution Strategy: MODE={MODE}, SELECTION={SELECTION}, CV={CV}\n")


    """ Preprocessing """
    P = Preprocessor(dirPath="data/")
    P.process()

    """ Modelling """
    C = Classifier(cv=CV,
                   preprocessor=P,
                   classifiers=classifiers,
                   evals=evals,
                   MODE=MODE)
    C.classify()
            risk = "VERY LOW RISK"
            print(G)

        # If needed, print an indication message to report the risk to users
        if info:
            print(f"\n>> Detection result: {meanEval} - {risk} for SQL INJECTION \n(predicted by the weighted scanner depends on {len(detectors)} models)\n")
        print(RESET)
        return meanEval, risk

    def file(self):
        """
            Scans and evaluates each line in a log file for detecting SQL injection risks. This method is designed for
        a batch processing of many SQL semantics acquired by a website; The input data is assumed to be stored in a log
        file line by line;
        """
        # Output indication information to ask for a log file path input
        print(">> Ready for file-based joint scanning.")
        print(">> Enter your file path to scan and evaluate")
        filepath = str(input("> ").strip())

        # Prepare a report table for reporting the batch-based scanning outcomes
        recorder = PrettyTable(['Index', 'Source Code', 'Score', 'Risk Level'])
        fileEval = 0
        try:
            # Report the risk level for each line with the corresponding colour and message output
            with open(filepath, 'r') as f:
                print(f">> Scanning File: {filepath}...")
                for (index, line) in enumerate(f):
                    print(f"\r* Checking line {index}")
                    lineEval, riskLevel = self.scan(line)
                    if float(lineEval[0]) >= 0.7:
                        color = R
                    elif float(lineEval[0]) >= 0.6:
                        color = Y
                    else:
                        color = G
                    fileEval += lineEval
                    recorder.add_row([color + str(index), line, lineEval, riskLevel])
        # If filepath invalid, report the issue and stop the file scanning
        except FileNotFoundError:
            print(f">> FilePathError: '{filepath}' not found, please try again;\n")
            return
        print(f">> Scanning Accomplished.\n")

        # Also compute and report an overall risk level based on all datalines contained in the log file
        size = len(recorder._rows)
        fileEval /= size
        if fileEval >= 0.9:
            risk = "VERY HIGH SQL INJECTION RISK"
        elif fileEval >= 0.7:
            risk = "HIGH SQL INJECTION RISK"
        elif fileEval >= 0.6:
            risk = "MEDIUM SQL INJECTION RISK"
        elif fileEval >= 0.4:
            risk = "LOW SQL INJECTION RISK"
        else:
            risk = "VERY LOW SQL INJECTION RISK"
        print(f"\n>> Overall File Risk Score: {fileEval} - {risk} \n(predicted by the weighted file scanner over {size} lines of codes)\n")

        # Ask for the printing of the table with line-by-line risk detection outcomes
        print(">> Line-based result needed? [(y)es for printing]")
        ask = str(input("> ").strip().lower())
        if ask == "y" or ask == "yes":
            print(B)
            print(recorder)
            print(RESET)
            
def web_scan(input_text):
    """
        Scans an input text for SQL injection risks, to be used in a web application;
     :return:
        - formatted_output, dict, a dictionary contains the risk level and score;
     """
    # Create an instant of the Interactor, process the scanning over the input semantics
    agent = Interactor()
    meanEval, risk = agent.scan(input_text, info=False)

    # Organize the detection outcomes as a dictionary and return
    formatted_output = {
        "risk": risk,
        "score": float(meanEval)  # Converting to float for JSON serialization
    }
    return formatted_output


if __name__ == '__main__':
    """ 
        Here is the main process of the interactive detector;
    """
    # Initialize the interactor detector and start by calling Interactor.help()
    agent = Interactor()
    agent.help(init=True)

    # Enter the interactive process; Take a command option input and provide corresponding services by calling methods
    while True:
        cmd = input("> ").lower().strip()
        if cmd == "help" or cmd == "h":
            agent.help(init=True)
        elif cmd == "detect" or cmd == "d":
            agent.detect()
            agent.help()
        elif cmd == "scan" or cmd == "s":
            agent.scan()
            agent.help()
        elif cmd == "file" or cmd == "f":
            agent.file()
            agent.help()
        elif cmd == "rebuild" or cmd == "r":
            agent.rebuild()
            agent.help()
        # Enter the specific command to exit
        elif cmd == "exit" or cmd == "e":
            print(">> Have a good one!")
            exit()
        # If the input is invalid then give a response and continue the loop
        else:
            sys.stderr.write(">> Unknown Command, please review the instruction and try again\n")
