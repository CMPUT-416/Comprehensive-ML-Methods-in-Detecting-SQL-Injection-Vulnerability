import os
import sys
import numpy as np

from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from nltk import word_tokenize
from tqdm import tqdm
from torch import Tensor

from models.knn import KNNModel
from models.nb import NBModel
from models.logReg import LogRegModel
from models.svm import SVMModel
from models.baseline import BaselineModel
from models.lstm import LSTMModel
from models.decision_tree import DecisionTreeModel


class Classifier:
    def __init__(self, cv: int, preprocessor, classifiers: list, evals: tuple, MODE):
        self.preprocessor = preprocessor
        self.cv: int = cv
        self.evals: tuple = evals
        self.classifiers: list = classifiers
        self.MODE = MODE

    def classify(self):
        print("+" * 60)
        print("\t\t\t\t** MODEL LEARNING **")
        print("+" * 60 + '\n')

        print(f">> Training Settings"
              f"\n\t> Method of {self.cv}-fold Cross-validation"
              f"\n\t> Benchmarks of {self.evals}")

        print(f">> Modelling & Predictions")
        for classifier in tqdm(self.classifiers):
            print('\n')
            if self.MODE == "PRE":
                classifier.classify(self.preprocessor.datasets_train, self.preprocessor.datasets_test, False)
            else:
                classifier.classify(self.preprocessor.datasets_train, self.preprocessor.datasets_test, True)

        print('\n' + "+" * 60)
        print("\t\t\t** MODEL PREDICTION ACCOMPLISHED **")
        print("+" * 60 + '\n\n')


class Preprocessor:
    def __init__(self, dirPath="data/", d2vModel=None):
        self.d2vModel = d2vModel
        self.datasets_train = {}
        self.datasets_test = {}
        self.dirPath = dirPath

    def process(self):
        print("+" * 60)
        print("\t\t\t\t** DATA PREPROCESSING **")
        print("+" * 60 + '\n')

        self._dataVectorization()
        data: tuple = self._dataInput()
        self._dataSplitting(data, ratio=0.1)
        #self._dataScaling()

        print('\n' + "+" * 60)
        print("\t\t\t** DATA PREPROCESS ACCOMPLISHED **")
        print("+" * 60 + '\n\n')

    def _dataNormalization(self, doc):
        return word_tokenize(doc.strip())

    def _dataVectorization(self, vector_size=100, window=5, min_count=1, workers=4):
        print(f">> Data Vectorization"
              f"\n\t> Method of Word2Vec/Doc2Vec")
        documents = []
        for filename in os.listdir(self.dirPath):
            if filename[-8:] == 'sqli.txt':
                print(f"\t\t* Converting dataset - {filename}...")
                filePath = os.path.join(self.dirPath, filename)
                with open(filePath, 'r') as f:
                    for index, dataline in enumerate(f):
                        tokens = self._dataNormalization(dataline)
                        documents.append(TaggedDocument(words=tokens, tags=[index]))
        self.d2vModel = Doc2Vec(documents, vector_size=vector_size, window=window, min_count=min_count, workers=workers)
        self.d2vModel.save("srcs/doc2vec.pkl")

    def _dataInput(self):
        print(f">> Data Input")
        features_hashtable = []
        labels_hashtable = []

        files = os.listdir(self.dirPath)
        txt_files = [f for f in files if f.endswith('.txt')]
        assert len(txt_files) % 2 == 0
        txt_files.sort()

        for i in range(0, len(txt_files), 2):
            f1 = os.path.join(self.dirPath, txt_files[i])
            f2 = os.path.join(self.dirPath, txt_files[i + 1])
            if f1.split("-")[1] == "sqli.txt":
                feature_file = f1
                label_file = f2
            else:
                feature_file = f2
                label_file = f1

            with open(feature_file, 'r') as fF, open(label_file, 'r') as fL:
                for index, (feature, label) in enumerate(zip(fF, fL)):
                    tokens = self._dataNormalization(feature)
                    vector = self.d2vModel.infer_vector(tokens)
                    features_hashtable.append(vector)
                    labels_hashtable.append(int(label.strip()))
        return np.array(features_hashtable), np.array(labels_hashtable)

    def _dataSplitting(self, datasets: tuple, ratio=0.1):
        print(f">> Data Splitting")
        X_train, X_test, y_train, y_test = train_test_split(datasets[0], datasets[1], test_size=ratio, shuffle=True)
        self.datasets_train = [X_train.astype(float), y_train.astype(int)]
        self.datasets_test = [X_test.astype(float), y_test.astype(int)]
        print(f"\t> Data Loading:"
              f"\n\t      * datasets_train"
              f"\n\t\t        --> X: {self.datasets_train[0].shape}, y: {self.datasets_train[1].shape}"
              f"\n\t      * datasets_test"
              f"\n\t\t        --> X: {self.datasets_test[0].shape}, y: {self.datasets_test[1].shape}")

    def _dataScaling(self):
        print(f">> Data Scaling")
        scaler = StandardScaler().fit(self.datasets_train[0])
        self.datasets_train[0] = scaler.transform(self.datasets_train[0])
        self.datasets_test[0] = scaler.transform(self.datasets_test[0])



def main(argv):
    """ Main Func """
    # Possible cmd:
    #       python detector.py [MODE:<NEW/PRE>] [SELECTION:<*/0~8>] [CV:<2~5>]
    # 0=Baseline;
    # 1=LogisticRegression;
    # 2=Naive Bayes;
    # 3=k-nearest Neighbors;
    # 4=Support Vector Machine;
    # 5=FCN;
    # 6=RNN/LSTM;



    """ Dashboard """
    MODE = "NEW"
    SELECTION = '*'
    CV = 3
    evals = ('neg_mean_squared_error', 'f1_micro', 'balanced_accuracy')
    """ Dashboard """



    try:
        if argv[2] is not None:
            MODE = argv[1]
            assert MODE == "NEW" or MODE == "PRE"
        if argv[2] is not None:
            SELECTION = argv[2]
        if argv[3] is not None:
            CV = int(argv[3])
        else:
            raise
    except:
        MODE = MODE
        SELECTION =  SELECTION
        CV = CV
        #sys.stderr.write("\n>> CommandError: Invalid command entered, please check the command and retry;")
        #exit()

    # Initialize new models:
    classifiers = []
    if SELECTION == '*':
        classifiers = [LogRegModel(cv=CV, cvEval=evals),
                       NBModel(cv=CV, cvEval=evals),
                       KNNModel(cv=CV, cvEval=evals),
                       SVMModel(cv=CV, cvEval=evals),
                       DecisionTreeModel(cv=CV, cvEval=evals)]
    elif SELECTION == 0:
        pass
    elif SELECTION == 1:
        classifiers.append(LogRegModel(cv=CV, cvEval=evals))
    elif SELECTION == 2:
        classifiers.append(NBModel(cv=CV, cvEval=evals))
    elif SELECTION == 3:
        classifiers.append(KNNModel(cv=CV, cvEval=evals))
    elif SELECTION == 4:
        classifiers.append(SVMModel(cv=CV, cvEval=evals))
    elif SELECTION == 5:
        pass
    elif SELECTION == 6:
        classifiers.append(DecisionTreeModel(cv=CV, cvEval=evals))
    else:
        sys.stderr.write("\n>> ModelSelectionError: Invalid model selection, valid scope is [0-6];")
        exit()

    # Apply pre-trained models if needed:
    if MODE == "NEW":
        for classifier in classifiers:
            classifier._reload()
    print(f"\n>> Execution Strategy: MODE={MODE}, SELECTION={SELECTION}, CV={CV}\n")


    """ Preprocessing """
    P = Preprocessor(dirPath="data/")
    P.process()

    """ Modelling """
    C = Classifier(cv=CV,
                   preprocessor=P,
                   classifiers=classifiers,
                   evals=evals,
                   MODE=MODE)
    C.classify()


if __name__ == '__main__':
    main(sys.argv)