import os
import numpy as np

from gensim.models.doc2vec import TaggedDocument, Doc2Vec
from nltk import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


class Preprocessor:
    def __init__(self, dirPath="data/", d2vModel=None):
        self.d2vModel = d2vModel
        self.datasets_train = {}
        self.datasets_test = {}
        self.dirPath = dirPath

    def process(self):
        print("+" * 60)
        print("\t\t\t\t** DATA PREPROCESSING **")
        print("+" * 60 + '\n')

        self._dataVectorization()
        data: tuple = self._dataInput()
        self._dataSplitting(data, ratio=0.1)
        #self._dataScaling()

        print('\n' + "+" * 60)
        print("\t\t\t** DATA PREPROCESS ACCOMPLISHED **")
        print("+" * 60 + '\n\n')

    def _dataNormalization(self, doc):
        return word_tokenize(doc.strip())

    def _dataVectorization(self, vector_size=100, window=5, min_count=1, workers=4):
        print(f">> Data Vectorization"
              f"\n\t> Method of Word2Vec/Doc2Vec")
        documents = []
        for filename in os.listdir(self.dirPath):
            if filename[-8:] == 'sqli.txt':
                print(f"\t\t* Converting dataset - {filename}...")
                filePath = os.path.join(self.dirPath, filename)
                with open(filePath, 'r') as f:
                    for index, dataline in enumerate(f):
                        tokens = self._dataNormalization(dataline)
                        documents.append(TaggedDocument(words=tokens, tags=[index]))
        self.d2vModel = Doc2Vec(documents, vector_size=vector_size, window=window, min_count=min_count, workers=workers)
        self.d2vModel.save("srcs/doc2vec.pkl")

    def _dataInput(self):
        print(f">> Data Input")
        features_hashtable = []
        labels_hashtable = []

        files = os.listdir(self.dirPath)
        txt_files = [f for f in files if f.endswith('.txt')]
        assert len(txt_files) % 2 == 0
        txt_files.sort()

        for i in range(0, len(txt_files), 2):
            f1 = os.path.join(self.dirPath, txt_files[i])
            f2 = os.path.join(self.dirPath, txt_files[i + 1])
            if f1.split("-")[1] == "sqli.txt":
                feature_file = f1
                label_file = f2
            else:
                feature_file = f2
                label_file = f1

            with open(feature_file, 'r') as fF, open(label_file, 'r') as fL:
                for index, (feature, label) in enumerate(zip(fF, fL)):
                    tokens = self._dataNormalization(feature)
                    vector = self.d2vModel.infer_vector(tokens)
                    features_hashtable.append(vector)
                    labels_hashtable.append(int(label.strip()))
        return np.array(features_hashtable), np.array(labels_hashtable)

    def _dataSplitting(self, datasets: tuple, ratio=0.1):
        print(f">> Data Splitting")
        X_train, X_test, y_train, y_test = train_test_split(datasets[0], datasets[1], test_size=ratio, shuffle=True)
        self.datasets_train = [X_train.astype(float), y_train.astype(int)]
        self.datasets_test = [X_test.astype(float), y_test.astype(int)]
        print(f"\t> Data Loading:"
              f"\n\t      * datasets_train"
              f"\n\t\t        --> X: {self.datasets_train[0].shape}, y: {self.datasets_train[1].shape}"
              f"\n\t      * datasets_test"
              f"\n\t\t        --> X: {self.datasets_test[0].shape}, y: {self.datasets_test[1].shape}")

    def _dataScaling(self):
        print(f">> Data Scaling")
        scaler = StandardScaler().fit(self.datasets_train[0])
        self.datasets_train[0] = scaler.transform(self.datasets_train[0])
        self.datasets_test[0] = scaler.transform(self.datasets_test[0])