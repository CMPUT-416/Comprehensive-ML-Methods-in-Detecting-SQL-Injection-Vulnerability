import os
import numpy as np

from gensim.models.doc2vec import TaggedDocument, Doc2Vec
from nltk import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


class Preprocessor:
    def __init__(self, dirPath="data/", d2vModel=None):
        """
            Initializes a new instance of the data Preprocessor;
        :param
            - dirPath, str, the directory path where the data files are stored, "data/" by default;
            - d2vModel, gensim.models.Doc2Vec, a Doc2Vec model used for data vectorization;
        """
        # Preprocessing Settings
        self.d2vModel = d2vModel
        self.datasets_train = {}
        self.datasets_test = {}
        self.dirPath = dirPath

    def process(self):
        """
            As a controller method that will automatically perform the complete preprocessing which includes data
        input, vectorization, tokenization, normalization, scaling, and splitting;
        """
        # Outputs indication information
        print("+" * 60)
        print("\t\t\t\t** DATA PREPROCESSING **")
        print("+" * 60 + '\n')

        # Take the whole process for data preprocessing;
        self._dataVectorization()
        data: tuple = self._dataInput()
        self._dataSplitting(data, ratio=0.1)
        #self._dataScaling()

        # Outputs indication information
        print('\n' + "+" * 60)
        print("\t\t\t** DATA PREPROCESS ACCOMPLISHED **")
        print("+" * 60 + '\n\n')

    def _dataNormalization(self, doc: str):
        """
            This method is used to normalize the input document data;
        :param
            - doc, str, the document to be normalized;
        :return
            - the normalized document with invalid space removed;
        """
        # Remove invalid space and tokenize
        return word_tokenize(doc.strip())

    def _dataVectorization(self, vector_size=100, window=5, min_count=1, workers=4):
        """
            This method is used to create a word embedding model and vectorize the input document data;
        :param
            - vector_size, int, the dimensionality of the vectors;
            - window, int, the maximum distance between the current and predicted word within a sentence;
            - min_count, int, ignores all words with total frequency lower than this;
            - workers, int, the number of worker threads used to train the model.
        """
        # Outputs indication information
        print(f">> Data Vectorization"
              f"\n\t> Method of Word2Vec/Doc2Vec")

        # Read and collect all pairs of datasets in; Clean the data by tokenizing, normalizing and tagging
        documents = []
        for filename in os.listdir(self.dirPath):
            if filename[-8:] == 'sqli.txt':
                print(f"\t\t* Converting dataset - {filename}...")
                filePath = os.path.join(self.dirPath, filename)
                with open(filePath, 'r', encoding='utf-8') as f:
                    for index, dataline in enumerate(f):
                        tokens = self._dataNormalization(dataline)
                        documents.append(TaggedDocument(words=tokens, tags=[index]))

        # Convert the token inputs to a Doc2Vec model for vectorization, save the model locally
        self.d2vModel = Doc2Vec(documents=documents,
                                vector_size=vector_size,
                                window=window,
                                min_count=min_count,
                                workers=workers)
        self.d2vModel.save("srcs/doc2vec.pkl")

    def _dataInput(self):
        """
            This method is used to load the dataset from a specified directory path, the method will automatically
        find the file for features and labels, then collect them together to form the input dataset; Then take data
        preprocessing approaches over it to have usable collections for all features and all labels;
        :return
            - tuple of np.array, containing the feature array and the label array.
        """
        print(f">> Data Input")
        # Read in TXT files from the given directory path, check (must in-pair) and sort files for easy processing
        features_hashtable = []
        labels_hashtable = []
        files = os.listdir(self.dirPath)
        txt_files = [f for f in files if f.endswith('.txt')]
        assert len(txt_files) % 2 == 0
        txt_files.sort()

        # Find the in-pair files for features and labels automatically
        for i in range(0, len(txt_files), 2):
            f1 = os.path.join(self.dirPath, txt_files[i])
            f2 = os.path.join(self.dirPath, txt_files[i + 1])
            if f1.split("-")[1] == "sqli.txt":
                feature_file = f1
                label_file = f2
            else:
                feature_file = f2
                label_file = f1

            # Read and preprocess data, then collect them into parallel arrays for features and labels respectively
            with open(feature_file, 'r', encoding='utf-8') as fF, open(label_file, 'r', encoding='utf-8') as fL:
                for index, (feature, label) in enumerate(zip(fF, fL)):
                    tokens = self._dataNormalization(feature)
                    vector = self.d2vModel.infer_vector(tokens)
                    features_hashtable.append(vector)
                    labels_hashtable.append(int(label.strip()))
        return np.array(features_hashtable), np.array(labels_hashtable)

    def _dataSplitting(self, datasets: tuple, ratio: float=0.1):
        """
            This method is used to split the data into training and testing sets depends on the ratio demand;
        :param
            - datasets, tuple, contains the feature array and the label array;
            - ratio, float, represents the proportion of the dataset to include in the test split, 0.1 by default.
        """
        # Split all data into two sets of training (1-ratio) and testing (ratio) and reorganize the data structure
        print(f">> Data Splitting")
        X_train, X_test, y_train, y_test = train_test_split(datasets[0], datasets[1], test_size=ratio, shuffle=True)
        self.datasets_train = [X_train.astype(float), y_train.astype(int)]
        self.datasets_test = [X_test.astype(float), y_test.astype(int)]

        # Outputs indication information
        print(f"\t> Data Loading:"
              f"\n\t      * datasets_train"
              f"\n\t\t        --> X: {self.datasets_train[0].shape}, y: {self.datasets_train[1].shape}"
              f"\n\t      * datasets_test"
              f"\n\t\t        --> X: {self.datasets_test[0].shape}, y: {self.datasets_test[1].shape}")

    def _dataScaling(self):
        """
            This method is used to scale the data using standard scaler. This is an optional part for preprocessing;
        """
        print(f">> Data Scaling")
        # Create a StandardScaler based on the training features, process the scaling on both datasets
        scaler = StandardScaler().fit(self.datasets_train[0])
        self.datasets_train[0] = scaler.transform(self.datasets_train[0])
        self.datasets_test[0] = scaler.transform(self.datasets_test[0])
